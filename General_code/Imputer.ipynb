{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd\n",
    "# import selector as selector\n",
    "# from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "# import waterfall_chart\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# from ruamel.yaml import timestamp\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFECV, SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_curve, auc, precision_score, make_scorer, log_loss, roc_auc_score\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from sklearn import metrics, datasets, clone\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, validation_curve, \\\n",
    "    learning_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_1: \n",
      " (114, 6720)\n",
      "df_2: \n",
      " (76, 6720)\n"
     ]
    }
   ],
   "source": [
    "########################################  import original radiomics data  ################################################\n",
    "NHT_filepath = r'E:\\Pystudy\\Thyroid_P_N\\data\\results\\data_use\\mix\\imputer\\NHT_radiomics.xlsx'\n",
    "PTMC_filepath = r'E:\\Pystudy\\Thyroid_P_N\\data\\results\\data_use\\mix\\imputer\\PTMC_radiomics.xlsx'\n",
    "writer_1 = pd.ExcelWriter(NHT_filepath, engine='openpyxl', mode='a')\n",
    "writer_2 = pd.ExcelWriter(PTMC_filepath, engine='openpyxl', mode='a')\n",
    "df_1 = pd.read_excel(NHT_filepath)\n",
    "df_2 = pd.read_excel(PTMC_filepath)\n",
    "\n",
    "# NHT：df_1 to DataFrame\n",
    "df_1_colNames = df_1.columns\n",
    "df_1 = df_1.astype(np.float64)\n",
    "df_1 = pd.DataFrame(df_1)\n",
    "df_1.columns = df_1_colNames\n",
    "print('df_1:', \"\\n\",df_1.shape)\n",
    "\n",
    "# PTMC：df_2 to DataFrame\n",
    "df_2_colNames = df_2.columns\n",
    "df_2 = df_2.astype(np.float64)\n",
    "df_2 = pd.DataFrame(df_2)\n",
    "df_2.columns = df_2_colNames\n",
    "print('df_2:', \"\\n\",df_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHT Columns with missing values:\n",
      "T1_C_wavelet-HLH_firstorder_RobustMeanAbsoluteDeviation            8\n",
      "T1_C_log-sigma-3-0-mm-3D_firstorder_RobustMeanAbsoluteDeviation    8\n",
      "T1_C_wavelet-LLL_firstorder_RobustMeanAbsoluteDeviation            8\n",
      "T1_C_log-sigma-5-0-mm-3D_firstorder_RobustMeanAbsoluteDeviation    8\n",
      "T1_C_wavelet-LLH_firstorder_RobustMeanAbsoluteDeviation            8\n",
      "                                                                  ..\n",
      "T2_wavelet-LLH_glcm_DifferenceAverage                              1\n",
      "T2_wavelet-LLH_glcm_DifferenceEntropy                              1\n",
      "T2_wavelet-LLH_glcm_DifferenceVariance                             1\n",
      "T2_wavelet-LLH_glcm_JointEnergy                                    1\n",
      "T2_wavelet-LLL_gldm_SmallDependenceLowGrayLevelEmphasis            1\n",
      "Length: 2924, dtype: int64\n",
      "Total missing values in df_1: 12557\n",
      "NHT组学数据整体数量： 766080\n",
      "Percentage of missing values in NHT: 1.64%\n"
     ]
    }
   ],
   "source": [
    "######################################  NHT null check and statistics  ##################################################\n",
    "# check all null\n",
    "# df_1_Nan=df_1.isnull().sum()\n",
    "# df_2_Nan=df_2.isnull().sum()\n",
    "# print('df_1_NHT缺失值：',\"\\n\", df_1_Nan,\"\\n\")\n",
    "# print('df_2_PTMC缺失值：',\"\\n\", df_2_Nan,\"\\n\")\n",
    "\n",
    "# display columns of null and show total null number\n",
    "NHT_missing_data = df_1.isnull().sum()\n",
    "NHT_missing_data = NHT_missing_data[NHT_missing_data > 0].sort_values(ascending=False)\n",
    "total_missing_values = NHT_missing_data.sum()\n",
    "print(\"NHT Columns with missing values:\")\n",
    "print(NHT_missing_data)\n",
    "print(f\"Total missing values in df_1: {total_missing_values}\")\n",
    "\n",
    "# show number of NHT DataFrame and Percentage of missing values in NHT\n",
    "total_data_points = df_1.size\n",
    "print(\"NHT组学数据整体数量：\", total_data_points) \n",
    "missing_percentage = (total_missing_values / total_data_points) * 100\n",
    "print(f\"Percentage of missing values in NHT: {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTMC Columns with missing values:\n",
      "T1_C_original_shape_Elongation                          5\n",
      "T1_C_wavelet-LLH_glrlm_LongRunEmphasis                  5\n",
      "T1_C_wavelet-LLH_glszm_GrayLevelNonUniformity           5\n",
      "T1_C_wavelet-LLH_glrlm_ShortRunLowGrayLevelEmphasis     5\n",
      "T1_C_wavelet-LLH_glrlm_ShortRunHighGrayLevelEmphasis    5\n",
      "                                                       ..\n",
      "T1_wavelet-HLH_glcm_ClusterProminence                   3\n",
      "T1_wavelet-HLH_glcm_JointAverage                        3\n",
      "T1_wavelet-HLH_glcm_Autocorrelation                     3\n",
      "T1_wavelet-HLH_firstorder_Variance                      3\n",
      "T1_original_shape_Elongation                            3\n",
      "Length: 1920, dtype: int64\n",
      "Total missing values in df_1: 7680\n",
      "PTMC组学数据整体数量： 510720\n",
      "Percentage of missing values in PTMC: 1.50%\n"
     ]
    }
   ],
   "source": [
    "######################################  PTMC null check and statistics ##################################################\n",
    "# check all null\n",
    "# df_1_Nan=df_1.isnull().sum()\n",
    "# df_2_Nan=df_2.isnull().sum()\n",
    "# print('df_1_NHT缺失值：',\"\\n\", df_1_Nan,\"\\n\")\n",
    "# print('df_2_PTMC缺失值：',\"\\n\", df_2_Nan,\"\\n\")\n",
    "\n",
    "# display columns of null and show total null number\n",
    "PTMC_missing_data = df_2.isnull().sum()\n",
    "PTMC_missing_data = PTMC_missing_data[PTMC_missing_data > 0].sort_values(ascending=False)\n",
    "total_missing_values = PTMC_missing_data.sum()\n",
    "print(\"PTMC Columns with missing values:\")\n",
    "print(PTMC_missing_data)\n",
    "print(f\"Total missing values in df_1: {total_missing_values}\")\n",
    "\n",
    "# show number of PTMC DataFrame and Percentage of missing values in PTMC\n",
    "total_data_points = df_2.size \n",
    "print(\"PTMC组学数据整体数量：\", total_data_points) \n",
    "missing_percentage = (total_missing_values / total_data_points) * 100\n",
    "print(f\"Percentage of missing values in PTMC: {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_1缺失值处理后： \n",
      " ADC_original_shape_Elongation                               0\n",
      "ADC_original_shape_Flatness                                 0\n",
      "ADC_original_shape_LeastAxisLength                          0\n",
      "ADC_original_shape_MajorAxisLength                          0\n",
      "ADC_original_shape_Maximum2DDiameterColumn                  0\n",
      "                                                           ..\n",
      "T2_wavelet-LLL_gldm_LargeDependenceLowGrayLevelEmphasis     0\n",
      "T2_wavelet-LLL_gldm_LowGrayLevelEmphasis                    0\n",
      "T2_wavelet-LLL_gldm_SmallDependenceEmphasis                 0\n",
      "T2_wavelet-LLL_gldm_SmallDependenceHighGrayLevelEmphasis    0\n",
      "T2_wavelet-LLL_gldm_SmallDependenceLowGrayLevelEmphasis     0\n",
      "Length: 6720, dtype: int64 \n",
      "\n",
      "df_2缺失值处理后： \n",
      " ADC_original_shape_Elongation                               0\n",
      "ADC_original_shape_Flatness                                 0\n",
      "ADC_original_shape_LeastAxisLength                          0\n",
      "ADC_original_shape_MajorAxisLength                          0\n",
      "ADC_original_shape_Maximum2DDiameterColumn                  0\n",
      "                                                           ..\n",
      "T2_wavelet-LLL_gldm_LargeDependenceLowGrayLevelEmphasis     0\n",
      "T2_wavelet-LLL_gldm_LowGrayLevelEmphasis                    0\n",
      "T2_wavelet-LLL_gldm_SmallDependenceEmphasis                 0\n",
      "T2_wavelet-LLL_gldm_SmallDependenceHighGrayLevelEmphasis    0\n",
      "T2_wavelet-LLL_gldm_SmallDependenceLowGrayLevelEmphasis     0\n",
      "Length: 6720, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Missing Values using Imputer （Simple Imputer）\n",
    "imp = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "imp = imp.fit(df_1)\n",
    "\n",
    "df_1=imp.transform(df_1)\n",
    "df_2=imp.transform(df_2)\n",
    "df_1=pd.DataFrame(data=df_1, columns=df_1_colNames)\n",
    "df_2=pd.DataFrame(data=df_2, columns=df_2_colNames)\n",
    "\n",
    "df_1_Nan_imp=df_1.isnull().sum()\n",
    "df_2_Nan_imp=df_2.isnull().sum()\n",
    "print('df_1缺失值处理后：',\"\\n\", df_1_Nan_imp,\"\\n\") \n",
    "print('df_2缺失值处理后：',\"\\n\", df_2_Nan_imp,\"\\n\") \n",
    "\n",
    "df_1.to_excel(writer_1, sheet_name='NHT_imputer2', index=False)\n",
    "writer_1.save()\n",
    "writer_1.close()\n",
    "df_2.to_excel(writer_2, sheet_name='PTMC_imputer2', index=False)\n",
    "writer_2.save()\n",
    "writer_2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thyroid_jicheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
