{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest, shapiro, ttest_ind, mannwhitneyu, chi2_contingency, fisher_exact\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import shap\n",
    "from nri_f import nri\n",
    "from AUC_CI import auc_ci\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "from hyperopt import Trials\n",
    "from hyperopt import hp\n",
    "from shaphypetune import BoostSearch, BoostRFE, BoostRFA, BoostBoruta\n",
    "from copy import copy\n",
    "from Delong_test import DelongTest\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, validation_curve, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, KBinsDiscretizer\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "from sklearn import metrics, datasets, clone\n",
    "from pandas import set_option\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, precision_score, make_scorer, log_loss, roc_auc_score, mean_squared_error\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFECV, SelectFromModel\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib import pyplot\n",
    "import waterfall\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_selector import FeatureSelector\n",
    "import feature_selector as selector\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from statsmodels.tools import add_constant\n",
    "import statsmodels.stats.proportion as sm\n",
    "from statsmodels.formula.api import logit\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "from math import sqrt\n",
    "import math\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "import winsound\n",
    "duration = 500\n",
    "frequency = 440\n",
    "font_set = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_filepath = r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\non_split_chuyichangzhihou_all.xlsx'\n",
    "df_new = pd.read_excel(df_new_filepath, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_new['label']\n",
    "x = df_new\n",
    "\n",
    "test_size = 0.3\n",
    "seed = 42\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=test_size,\n",
    "    stratify=y,\n",
    "    shuffle=True,\n",
    "    random_state=seed)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "x_train.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_train_nonstd.xlsx', index=False)\n",
    "x_test.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_test_nonstd.xlsx', index=False)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_std_names = ['label', 'Name', 'Nodules',\t'Shape',\n",
    "                 'Margin',\t'Cystic',\t'ETE',\t'Enhancement', 'TI_RADS_score']\n",
    "colNames = x_train.columns\n",
    "std_names = colNames.drop(non_std_names)\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_train[std_names])\n",
    "\n",
    "x_train_sc = sc.transform(x_train[std_names])\n",
    "x_train[std_names] = pd.DataFrame(\n",
    "    x_train_sc, columns=std_names, index=x_train.index)\n",
    "x_train.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_train_std_all.xlsx', index=False)\n",
    "\n",
    "x_test_sc = sc.transform(x_test[std_names])\n",
    "x_test[std_names] = pd.DataFrame(\n",
    "    x_test_sc, columns=std_names, index=x_test.index)\n",
    "x_test.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_test_std_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradition_labes = ['MAD_mm', 'Nodules',\t'Shape', 'Margin',\t'Cystic',\t'ETE',\t'Enhancement',\t'T1_rCNR',\t'T1C_rCNR',\n",
    "                   'T1C_increase',\t'T2_rCNR',\t'B800_rCNR',\t'B2000_rCNR',\t'ADC_lesion',\t'ADC_normal',\t'ADC_rCNR',\t'TI_RADS_score']\n",
    "\n",
    "non_radiomics_labels = ['label', 'Name', 'MAD_mm', 'Nodules',\t'Shape', 'Margin',\t'Cystic',\t'ETE',\t'Enhancement',\t'T1_rCNR',\n",
    "                        'T1C_rCNR', 'T1C_increase',\t'T2_rCNR',\t'B800_rCNR',\t'B2000_rCNR',\t'ADC_lesion',\t'ADC_normal',\t'ADC_rCNR',\t'TI_RADS_score']\n",
    "\n",
    "radiomics_labels = colNames.drop(non_radiomics_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainT = x_train.loc[:, tradition_labes]\n",
    "x_testT = x_test.loc[:, tradition_labes]\n",
    "print(x_trainT.shape)\n",
    "print(x_testT.shape)\n",
    "print(' ')\n",
    "\n",
    "x_train_MRI = x_train[non_radiomics_labels]\n",
    "x_train_MRI.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_train_MRI.xlsx', index=False)\n",
    "x_test_MRI = x_test[non_radiomics_labels]\n",
    "x_test_MRI.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_test_MRI.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_test(df, cols):\n",
    "    results = []\n",
    "    for col in cols:\n",
    "\n",
    "        group0 = df[df['label'] == 0][col]\n",
    "        group1 = df[df['label'] == 1][col]\n",
    "\n",
    "        _, p0 = kstest(group0, 'norm')\n",
    "        _, p1 = kstest(group1, 'norm')\n",
    "\n",
    "        if p0 > 0.05 and p1 > 0.05:\n",
    "\n",
    "            t, p = ttest_ind(group0, group1)\n",
    "            test_type = 't-test'\n",
    "        else:\n",
    "\n",
    "            t, p = mannwhitneyu(group0, group1)\n",
    "            test_type = 'Mann-Whitney'\n",
    "\n",
    "        results.append((col, test_type, t, p))\n",
    "\n",
    "    return pd.DataFrame(results, columns=['variable', 'test_type', 'statistic', 'p_value'])\n",
    "\n",
    "\n",
    "def categorical_test(df, cols):\n",
    "    results = []\n",
    "    for col in cols:\n",
    "\n",
    "        table = pd.crosstab(df[col], df['label'])\n",
    "        if table.values.min() >= 5:\n",
    "            chi2, p, _, _ = chi2_contingency(table)\n",
    "            test_type = 'chi-square'\n",
    "        else:\n",
    "            _, p = fisher_exact(table)\n",
    "            test_type = 'Fisher'\n",
    "\n",
    "        results.append((col, test_type, None, p))\n",
    "\n",
    "    return pd.DataFrame(results, columns=['variable', 'test_type', 'statistic', 'p_value'])\n",
    "\n",
    "\n",
    "numeric_cols = ['MAD_mm', 'T1_rCNR', 'T1C_rCNR', 'T1C_increase', 'T2_rCNR',\n",
    "                'B800_rCNR', 'B2000_rCNR', 'ADC_lesion', 'ADC_normal', 'ADC_rCNR', 'TI_RADS_score']\n",
    "normality_res = normality_test(x_train_MRI, numeric_cols)\n",
    "\n",
    "\n",
    "categorical_cols = ['Nodules', 'Shape',\n",
    "                    'Margin', 'Cystic', 'ETE', 'Enhancement']\n",
    "categorical_res = categorical_test(x_train_MRI, categorical_cols)\n",
    "\n",
    "\n",
    "significant_vars = list(normality_res[normality_res['p_value'] < 0.05]['variable']) + \\\n",
    "    list(categorical_res[categorical_res['p_value'] < 0.05]['variable'])\n",
    "\n",
    "\n",
    "formula = 'label ~ ' + ' + '.join(significant_vars)\n",
    "model1 = logit(formula=formula, data=x_train_MRI).fit()\n",
    "print(\"\\n\", model1.summary())\n",
    "\n",
    "independent_vars = list(model1.pvalues[model1.pvalues < 0.05].index)\n",
    "x_train_MRI = add_constant(x_train_MRI)\n",
    "if 'Intercept' in independent_vars:\n",
    "    independent_vars.remove('Intercept')\n",
    "formula = 'label ~ ' + ' + '.join(independent_vars)\n",
    "clf_MRI = logit(formula=formula, data=x_train_MRI).fit()\n",
    "print(\"\\n\", clf_MRI.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_MRI_train_auc\n",
    "y_pred_proba = clf_MRI.predict(x_train_MRI[independent_vars])\n",
    "clf_MRI_train_auc = roc_auc_score(y_train, y_pred_proba)\n",
    "print(\"clf_MRI_train_auc:\", clf_MRI_train_auc)\n",
    "clf_MRI_train_auc_CI = auc_ci(y_true=y_train, y_score=clf_MRI.predict(\n",
    "    x_train_MRI[independent_vars]), positive=1)\n",
    "print(\"clf_MRI_train_auc_CI:\", clf_MRI_train_auc_CI)\n",
    "\n",
    "y_pred_proba = clf_MRI.predict(x_train_MRI[independent_vars])\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# clf_MRI Logit Training set confusion_matrix\n",
    "print(\"clf_MRI Logit Training set confusion_matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_train, y_pred=y_pred).ravel()\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# clf_MRI Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"clf_MRI Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"clf_MRI Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# clf_MRI Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"clf_MRI Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"clf_MRI Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# clf_MRI Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"clf_MRI Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"clf_MRI Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# clf_MRI_test_auc\n",
    "y_pred_proba = clf_MRI.predict(x_test_MRI[independent_vars])\n",
    "clf_MRI_test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"clf_MRI_test_auc:\", clf_MRI_test_auc)\n",
    "clf_MRI_test_auc_CI = auc_ci(y_true=y_test, y_score=clf_MRI.predict(\n",
    "    x_test_MRI[independent_vars]), positive=1)\n",
    "print(\"clf_MRI_test_auc_CI:\", clf_MRI_test_auc_CI)\n",
    "\n",
    "y_pred_proba = clf_MRI.predict(x_test_MRI[independent_vars])\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# clf_MRI Logit Test set confusion_matrix\n",
    "print(\"clf_MRI Logit Test set confusion_matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# clf_MRI Test set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"clf_MRI Test set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"clf_MRI Test set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# clf_MRI Test set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"clf_MRI Test set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"clf_MRI Test set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# clf_MRI Test set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"clf_MRI Test set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"clf_MRI Test set 95% CI (accuracy):\", ci_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_MRI testing set ROC curve\n",
    "def auc_MRI_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_MRI_CI_L = auc_MRI_ci_lower(y_true=y_test, y_score=clf_MRI.predict(\n",
    "    x_test_MRI[independent_vars]), positive=1)\n",
    "# print(\"MRI_AUC_95%CI_lower:\", AUC_MRI_CI_L)\n",
    "\n",
    "# auc_MRI 95% CI_upper：\n",
    "\n",
    "\n",
    "def auc_MRI_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_MRI_CI_U = auc_MRI_ci_upper(y_true=y_test, y_score=clf_MRI.predict(\n",
    "    x_test_MRI[independent_vars]), positive=1)\n",
    "# print(\"MRI_AUC_95%CI_upper:\", AUC_MRI_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_MRI.predict(x_test_MRI[independent_vars])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs, pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % clf_MRI_test_auc + '(%0.2f' % AUC_MRI_CI_L + '-' + '%0.2f)' % AUC_MRI_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Logit ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Logit_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_MRI calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_MRI.predict(x_test_MRI[independent_vars]), n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='Logit Model')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean predicted value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of positives\", fontsize=12)\n",
    "plt.title(\"Logit Model Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Logit_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_MRI DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_MRI.predict(x_test_MRI[independent_vars])\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('Logit Model Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Logit_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureSelector_1\n",
    "fs = FeatureSelector(data=x_train[radiomics_labels], labels=y_train)\n",
    "\n",
    "# correlation_threshold is 0.8\n",
    "fs.identify_collinear(correlation_threshold=0.8)\n",
    "collinear_features = fs.ops['collinear']\n",
    "print('collinear_features:', collinear_features[:10])\n",
    "print('collinear_features:', fs.record_collinear.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureSelector_2\n",
    "fs.identify_zero_importance(task='classification',\n",
    "                            eval_metric='auc',\n",
    "                            n_iterations=10,\n",
    "                            early_stopping=True)\n",
    "\n",
    "# list of zero importance features\n",
    "zero_importance_features = fs.ops['zero_importance']\n",
    "print('zero_importance_features:', zero_importance_features[:10])\n",
    "\n",
    "# one hot features\n",
    "one_hot_features = fs.one_hot_features\n",
    "base_features = fs.base_features\n",
    "print('There are %d original features' % len(base_features))\n",
    "print('There are %d one-hot features' % len(one_hot_features))\n",
    "\n",
    "fs.plot_feature_importances(threshold=0.99, plot_n=12)\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureSelector_3\n",
    "fs.identify_low_importance(cumulative_importance=0.99)\n",
    "low_importance_features = fs.ops['low_importance']\n",
    "print('low_importance_features:', low_importance_features[:10])\n",
    "\n",
    "selected_features = fs.remove(methods='all', keep_one_hot=False)\n",
    "fs_names = selected_features.columns\n",
    "\n",
    "print('selected_features；', selected_features.shape)\n",
    "print('fs_names:', fs_names[:10])\n",
    "print(x_train[radiomics_labels].shape[1])\n",
    "print(selected_features.shape[1])\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = non_radiomics_labels + list(fs_names)\n",
    "x_train_fs = x_train[selected_columns]\n",
    "x_test_fs = x_test[selected_columns]\n",
    "\n",
    "print(x_train_fs.shape)\n",
    "print(x_test_fs.shape)\n",
    "\n",
    "x_train_step1 = x_train_fs\n",
    "x_test_step1 = x_test_fs\n",
    "print(\"x_train_step1：\", x_train_step1.shape)\n",
    "print(\"x_test_step1：\", x_test_step1.shape)\n",
    "\n",
    "x_train_step1 = x_train_fs\n",
    "x_test_step1 = x_test_fs\n",
    "print(\"x_train_step1：\", x_train_step1.shape)\n",
    "print(\"x_test_step1：\", x_test_step1.shape)\n",
    "x_trainT = x_train_step1.loc[:, tradition_labes]\n",
    "x_testT = x_test_step1.loc[:, tradition_labes]\n",
    "print(x_trainT.shape)\n",
    "print(x_testT.shape)\n",
    "print(' ')\n",
    "\n",
    "x_train_MRI = x_train_step1[non_radiomics_labels]\n",
    "x_train_MRI.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_train_MRI.xlsx', index=False)\n",
    "x_test_MRI = x_test_step1[non_radiomics_labels]\n",
    "x_test_MRI.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_test_MRI.xlsx', index=False)\n",
    "\n",
    "x_train_radiomics = x_train_step1.loc[:, fs_names]\n",
    "x_test_radiomics = x_test_step1.loc[:, fs_names]\n",
    "print(x_train_radiomics.shape)\n",
    "print(x_test_radiomics.shape)\n",
    "\n",
    "radiomics_columns = ['label', 'Name'] + list(fs_names)\n",
    "x_train_radiomics_name = x_train_step1.loc[:, radiomics_columns]\n",
    "x_test_radiomics_name = x_test_step1.loc[:, radiomics_columns]\n",
    "x_train_radiomics_name.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_train__radiomics.xlsx', index=False)\n",
    "x_test_radiomics_name.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_test__radiomics.xlsx', index=False)\n",
    "\n",
    "x_train_sc = x_train_radiomics\n",
    "x_test_sc = x_test_radiomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO\n",
    "lr = LogisticRegression(penalty='l1', max_iter=100000,\n",
    "                        solver='liblinear', multi_class='ovr', random_state=42)\n",
    "penaltys = ['l1']\n",
    "Cs = np.logspace(-1, 1, 50)\n",
    "tuned_parameters = dict(penalty=penaltys, C=Cs)\n",
    "\n",
    "grid = GridSearchCV(lr, tuned_parameters, return_train_score=True, cv=10)\n",
    "\n",
    "grid.fit(x_train_sc, y_train)\n",
    "print('clf.best_score_:', grid.best_score_)\n",
    "print('grid.best_params_', grid.best_params_)\n",
    "print(' clf.best_estimator_:', grid.best_estimator_)\n",
    "regr_cv = grid.best_estimator_\n",
    "\n",
    "test_means = grid.cv_results_['mean_test_score']\n",
    "test_stds = grid.cv_results_['std_test_score']\n",
    "train_means = grid.cv_results_['mean_train_score']\n",
    "train_stds = grid.cv_results_['std_train_score']\n",
    "# plot results\n",
    "n_Cs = len(Cs)\n",
    "number_penaltys = len(penaltys)\n",
    "test_scores = np.array(test_means).reshape(n_Cs, number_penaltys)\n",
    "train_scores = np.array(train_means).reshape(n_Cs, number_penaltys)\n",
    "test_stds = np.array(test_stds).reshape(n_Cs, number_penaltys)\n",
    "train_stds = np.array(train_stds).reshape(n_Cs, number_penaltys)\n",
    "\n",
    "x_axis = np.log10(Cs)\n",
    "for i, value in enumerate(penaltys):\n",
    "    plt.errorbar(\n",
    "        x_axis, test_scores[:, i], yerr=test_stds[:, i], label=penaltys[i] + ' Test')\n",
    "    plt.errorbar(\n",
    "        x_axis, train_scores[:, i], yerr=train_stds[:, i], label=penaltys[i] + ' Train')\n",
    "plt.legend()\n",
    "plt.xlabel('C')\n",
    "plt.axvline(grid.best_params_['C'], color='black', ls=\"--\")\n",
    "plt.ylabel('Accuary')\n",
    "# plt.savefig('LogisticGridSearchCV_C.png')\n",
    "pyplot.show()\n",
    "\n",
    "lr = LogisticRegression(penalty='l1',\n",
    "                        C=grid.best_params_['C'],\n",
    "                        solver='liblinear',\n",
    "                        multi_class='ovr',\n",
    "                        random_state=42)\n",
    "\n",
    "lr.fit(x_train_sc, y_train)\n",
    "print('Train accuracy:', lr.score(x_train_sc, y_train))\n",
    "print('Test accuracy:', lr.score(x_test_sc, y_test))\n",
    "# print('lr.coef_:', lr.coef_)\n",
    "# print('lr.intercept_:', lr.intercept_)\n",
    "\n",
    "\n",
    "# SelectFromModel\n",
    "threshold = np.percentile(np.abs(lr.coef_), 75)\n",
    "selector = SelectFromModel(estimator=lr, threshold=None, prefit=False)\n",
    "selector.fit(x_train_sc, y_train)\n",
    "x_selected = selector.transform(x_train_sc)\n",
    "x_selected_test = selector.transform(x_test_sc)\n",
    "\n",
    "print('\\n', 'Number of features that meet this threshold criterion:',\n",
    "      x_selected.shape[1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = fs_names\n",
    "index = lr.coef_[lr.coef_ != 0]\n",
    "index = index.ravel()\n",
    "indices = np.argsort(index)[::-1]\n",
    "res_1, res_2 = [], []\n",
    "for f in range(x_selected.shape[1]):\n",
    "    index_ = index[indices[f]]\n",
    "    res_1.append(index_)\n",
    "    feat_labels_ = feat_labels[indices[f]]\n",
    "    res_2.append(feat_labels_)\n",
    "coef_series = pd.Series(res_1, index=np.array(res_2))\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print(coef_series)\n",
    "\n",
    "# coef_series to DataFrame\n",
    "coef_series_list = pd.concat([pd.Series(res_2), pd.Series(res_1)], axis=1)\n",
    "coef_series_list.columns = ['feature', 'coef']\n",
    "# coef_series_list to excel\n",
    "coef_series_list.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\coef_series_list.xlsx', index=False)\n",
    "\n",
    "fig = plt.figure(dpi=300)\n",
    "ax = plt.subplot(111)\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "coef_series.plot(kind=\"bar\", color='k', grid=True, alpha=0.5)\n",
    "plt.xticks(rotation=30, ha='right', va='top', fontsize=4)  # fontsize=5\n",
    "plt.title(\"Coefficients in Model\")\n",
    "plt.subplots_adjust(left=0.2, bottom=0.5, right=0.95, top=0.95)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "selected_column_names = coef_series.index\n",
    "x_selected = pd.DataFrame(x_train_sc, columns=selected_column_names)\n",
    "x_selected_test = pd.DataFrame(x_test_sc, columns=selected_column_names)\n",
    "\n",
    "x_selected.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_selected_train.xlsx', index=False)\n",
    "x_selected_test.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_selected_test.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=300)\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "weights, params = [], []\n",
    "for c in np.arange(-4., 6.):\n",
    "    lr = LogisticRegression(penalty='l1', C=10. ** c, solver='liblinear',\n",
    "                            multi_class='ovr', random_state=42)\n",
    "    lr.fit(x_train_sc, y_train)\n",
    "    weights.append(lr.coef_[0])\n",
    "    params.append(10 ** c)\n",
    "\n",
    "weights = np.array(weights)\n",
    "\n",
    "for column in range(weights.shape[1]):\n",
    "    plt.plot(params, weights[:, column],\n",
    "             label=x.columns[column])\n",
    "plt.axvline(grid.best_params_['C'], color='black', ls=\"--\")\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n",
    "plt.xlim([10 ** (-5), 10 ** 5])\n",
    "plt.ylabel('Coefficients Weights', fontproperties=font_set)\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "# plt.legend(loc='upper left')\n",
    "# ax.legend(loc='upper center',\n",
    "#           bbox_to_anchor=(1.38, 1.03),\n",
    "#           ncol=1, fancybox=True)\n",
    "# plt.savefig('images/04_07.png', dpi=300,\n",
    "#            bbox_inches='tight', pad_inches=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_mean = grid.cv_results_['mean_test_score']\n",
    "MSEs_std = grid.cv_results_['std_test_score']\n",
    "\n",
    "fig = plt.figure(dpi=300)\n",
    "ax = plt.subplot(111)\n",
    "plt.errorbar(np.log10(1 / Cs), MSEs_mean, yerr=MSEs_std, fmt=\"o\", ms=3,\n",
    "             mfc=\"r\", mec=\"r\", ecolor=\"lightblue\", elinewidth=2, capsize=4, capthick=1)\n",
    "plt.semilogx()\n",
    "plt.axvline(grid.best_params_['C'], color=\"black\", ls=\"--\")\n",
    "plt.xlabel(\"C\")\n",
    "# plt.xlim([10 ** (-1), 10 ** 1])\n",
    "plt.ylabel(\"MSE\")\n",
    "ax = plt.gca()\n",
    "y_major_locator = MultipleLocator(0.05)\n",
    "ax.yaxis.set_major_locator(y_major_locator)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_filepath = r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\coef_series_list.xlsx'\n",
    "selected_features = pd.read_excel(selected_features_filepath, sheet_name=0)\n",
    "print(selected_features)\n",
    "\n",
    "selected_features.columns = ['names', 'coef']\n",
    "selected_column_names = selected_features['names']\n",
    "print(selected_column_names)\n",
    "\n",
    "x_selected = pd.DataFrame(x_train, columns=selected_column_names)\n",
    "x_selected_test = pd.DataFrame(x_test, columns=selected_column_names)\n",
    "\n",
    "x_selected.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_selected_train.xlsx', index=False)\n",
    "x_selected_test.to_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\x_selected_test.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 42\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR model\n",
    "lr = LogisticRegression(C=grid.best_params_['C'], penalty='l1', max_iter=100000, solver='liblinear', multi_class='ovr', random_state=42)\n",
    "lr.fit(x_selected, y_train)\n",
    "\n",
    "# LR AUC in Training set\n",
    "LR_Training_AUC = roc_auc_score(y_train,lr.predict_proba(x_selected)[:, 1])\n",
    "print(\"LR_Training_AUC:\", LR_Training_AUC)\n",
    "LR_Training_AUC_CI = auc_ci(y_true=y_train, y_score=lr.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"LR_Training_AUC_95%CI:\", LR_Training_AUC_CI)\n",
    "\n",
    "# LR confusion_matrix in Training set\n",
    "print(\"LR confusion_matrix in Training set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_true = y_train, y_pred = lr.predict(x_selected)).ravel()\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# LR Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"LR Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"LR Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# LR Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"LR Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples=n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"LR Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# LR Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"LR Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp \n",
    "n_samples= tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"LR Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# LR AUC in Testing set\n",
    "LR_Testing_AUC = roc_auc_score(y_test,lr.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"LR_Testing_AUC:\", LR_Testing_AUC)\n",
    "LR_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=lr.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "print(\"LR_Testing_AUC_95%CI:\", LR_Testing_AUC_CI)\n",
    "\n",
    "# LR confusion_matrix in Testing set\n",
    "print(\"LR confusion_matrix in Testing set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = lr.predict(x_selected_test)).ravel()\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# LR Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"LR Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"LR Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# LR Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"LR Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"LR Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# LR Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"LR Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp \n",
    "n_samples= tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"LR Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# LR testing set ROC curve\n",
    "\n",
    "# auc_lr 95% CI_lower：\n",
    "def auc_lr_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) + (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_LR_CI_L = auc_lr_ci_lower(y_true=y_test, y_score=lr.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"LR_AUC_95%CI_lower:\", AUC_LR_CI_L)\n",
    "\n",
    "\n",
    "# auc_lr 95% CI_upper：\n",
    "def auc_lr_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) + (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_LR_CI_U = auc_lr_ci_upper(y_true=y_test, y_score=lr.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"LR_AUC_95%CI_upper:\", AUC_LR_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = lr.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % LR_Testing_AUC + '(%0.2f' % AUC_LR_CI_L + '-' + '%0.2f)' % AUC_LR_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('LR ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LR_ROC.png', dpi = 300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR SHAP\n",
    "explainer = shap.LinearExplainer(lr, x_selected)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"LR SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'LR_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, lr.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='LR Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"LR Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LR_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = lr.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('LR Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LR_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN tuning\n",
    "neighbors = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "param_grid = dict(n_neighbors=neighbors)\n",
    "KNN = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=KNN, param_grid=param_grid,\n",
    "                    scoring=scoring, cv=kfold)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "print('\\n', \"KNN Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# KNN fit training set\n",
    "clf_KNN = grid.best_estimator_\n",
    "clf_KNN.fit(x_selected, y_train)\n",
    "\n",
    "# KNN AUC in Training set\n",
    "KNN_Training_AUC = roc_auc_score(\n",
    "    y_train, clf_KNN.predict_proba(x_selected)[:, 1])\n",
    "print(\"KNN_Training_AUC:\", KNN_Training_AUC)\n",
    "KNN_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=clf_KNN.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"KNN_Training_AUC_95%CI:\", KNN_Training_AUC_CI)\n",
    "\n",
    "# KNN confusion_matrix in Training set\n",
    "print(\"KNN confusion_matrix in Training set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=clf_KNN.predict(x_selected)).ravel()\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# KNN Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"KNN Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"KNN Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# KNN Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"KNN Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"KNN Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# KNN Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"KNN Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"KNN Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# KNN AUC in Testing set\n",
    "KNN_Testing_AUC = roc_auc_score(\n",
    "    y_test, clf_KNN.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"KNN_Testing_AUC:\", KNN_Testing_AUC)\n",
    "KNN_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=clf_KNN.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"KNN_Testing_AUC_95%CI:\", KNN_Testing_AUC_CI)\n",
    "\n",
    "# KNN confusion_matrix in Testing set\n",
    "print(\"KNN confusion_matrix in Testing set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=clf_KNN.predict(x_selected_test)).ravel()\n",
    "print(\"Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# KNN Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"KNN Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"KNN Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# KNN Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"KNN Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"KNN Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# KNN Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"KNN Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"KNN Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# KNN Testing set ROC\n",
    "\n",
    "# auc_KNN 95% CI_lower：\n",
    "def auc_KNN_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_KNN_CI_L = auc_KNN_ci_lower(\n",
    "    y_true=y_test, y_score=clf_KNN.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "print(\"KNN_AUC_95%CI_lower:\", AUC_KNN_CI_L)\n",
    "\n",
    "\n",
    "# auc_KNN 95% CI_upper：\n",
    "def auc_KNN_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_KNN_CI_U = auc_KNN_ci_upper(\n",
    "    y_true=y_test, y_score=clf_KNN.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "print(\"KNN_AUC_95%CI_upper:\", AUC_KNN_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_KNN.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % KNN_Testing_AUC + '(%0.2f' % AUC_KNN_CI_L + '-' + '%0.2f)' % AUC_KNN_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('KNN ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\KNN_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN SHAP\n",
    "explainer = shap.KernelExplainer(clf_KNN.predict, x_selected)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"KNN SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'KNN_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_KNN.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='KNN Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"KNN Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\KNN_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_KNN.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('KNN Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\KNN_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm tuning\n",
    "Cs = [0.001, 0.005, 0.01, 0.05, 0.09, 0.1, 0.3, 0.5, 0.7,\n",
    "      0.9, 1.0, 1.3, 1.5, 1.7, 1.9, 2.1, 2.3, 2.5, 3.0]\n",
    "gammas = np.logspace(-4, 1, 10, base=2)\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "param_grid = dict(C=Cs, kernel=kernel_values, gamma=gammas)\n",
    "model = SVC(probability=True)\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                    scoring=scoring, cv=kfold)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "print('\\n', \"SVM Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# SVM fit training set\n",
    "clf_SVM = grid.best_estimator_\n",
    "clf_SVM.fit(x_selected, y_train)\n",
    "\n",
    "# SVM AUC in Training set\n",
    "SVM_Training_AUC = roc_auc_score(\n",
    "    y_train, clf_SVM.predict_proba(x_selected)[:, 1])\n",
    "print(\"SVM_Training_AUC:\", SVM_Training_AUC)\n",
    "SVM_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=clf_SVM.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"SVM_Training_AUC_95%CI:\", SVM_Training_AUC_CI)\n",
    "\n",
    "# SVM confusion_matrix in Training set\n",
    "print(\"SVM confusion_matrix in Training set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=clf_SVM.predict(x_selected)).ravel()\n",
    "print(\"SVM Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# SVM Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"SVM Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"SVM Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# SVM Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"SVM Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"SVM Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# SVM Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"SVM Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"SVM Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# SVM AUC in Testing set\n",
    "SVM_Testing_AUC = roc_auc_score(\n",
    "    y_test, clf_SVM.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"SVM_Testing_AUC:\", SVM_Testing_AUC)\n",
    "SVM_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=clf_SVM.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"SVM_Testing_AUC_95%CI:\", SVM_Testing_AUC_CI)\n",
    "\n",
    "# SVM confusion_matrix in Testing set\n",
    "print(\"SVM confusion_matrix in Testing set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=clf_SVM.predict(x_selected_test)).ravel()\n",
    "print(\"SVM Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# SVM Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"SVM Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"SVM Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# SVM Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"SVM Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"SVM Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# SVM Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"SVM Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"SVM Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# SVM test set ROC\n",
    "\n",
    "# auc_SVM 95% CI_lower：\n",
    "def auc_SVM_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_SVM_CI_L = auc_SVM_ci_lower(\n",
    "    y_true=y_test, y_score=clf_SVM.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"SVM_AUC_95%CI_lower:\", AUC_SVM_CI_L)\n",
    "\n",
    "\n",
    "# auc_SVM 95% CI_upper：\n",
    "def auc_SVM_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_SVM_CI_U = auc_SVM_ci_upper(\n",
    "    y_true=y_test, y_score=clf_SVM.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"SVM_AUC_95%CI_upper:\", AUC_SVM_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_SVM.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % SVM_Testing_AUC + '(%0.2f' % AUC_SVM_CI_L + '-' + '%0.2f)' % AUC_SVM_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('SVM ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\SVM_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM SHAP\n",
    "explainer = shap.KernelExplainer(clf_SVM.predict, x_selected)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"SVM SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'SVM_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_SVM.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='SVM Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"SVM Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\SVM_Cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_SVM.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('SVM Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\SVM_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA tuning\n",
    "solvers = ['svd', 'lsqr', 'eigen']\n",
    "shrinkages = [0.3, 0.5, 0.7, 0.9, 1.1]\n",
    "param_grid = dict(solver=solvers, shrinkage=shrinkages)\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=LDA, param_grid=param_grid,\n",
    "                    scoring=scoring, cv=kfold)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "# print('LDA的estimator.get_params():',grid.estimator.get_params().keys())\n",
    "print('\\n', \"LDA Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# LDA fit training set\n",
    "clf_LDA = grid.best_estimator_\n",
    "clf_LDA.fit(x_selected, y_train)\n",
    "\n",
    "# LDA AUC in Training set\n",
    "LDA_Training_AUC = roc_auc_score(\n",
    "    y_train, clf_LDA.predict_proba(x_selected)[:, 1])\n",
    "print(\"LDA_Training_AUC:\", LDA_Training_AUC)\n",
    "LDA_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=clf_LDA.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"LDA_Training_AUC_95%CI:\", LDA_Training_AUC_CI)\n",
    "\n",
    "# LDA confusion_matrix in Training set\n",
    "print(\"LDA confusion_matrix in Training set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=clf_LDA.predict(x_selected)).ravel()\n",
    "print(\"LDA Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# LDA Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"LDA Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"LDA Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# LDA Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"LDA Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"LDA Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# LDA Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"LDA Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"LDA Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# LDA AUC in Testing set\n",
    "LDA_Testing_AUC = roc_auc_score(\n",
    "    y_test, clf_LDA.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"LDA_Testing_AUC:\", LDA_Testing_AUC)\n",
    "LDA_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=clf_LDA.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"LDA_Testing_AUC_95%CI:\", LDA_Testing_AUC_CI)\n",
    "\n",
    "# LDA confusion_matrix in Testing set\n",
    "print(\"LDA confusion_matrix in Testing set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=clf_LDA.predict(x_selected_test)).ravel()\n",
    "print(\"LDA Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# LDA Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"LDA Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"LDA Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# LDA Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"LDA Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"LDA Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# LDA Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"LDA Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"LDA Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# LDA test ROC\n",
    "\n",
    "# auc_LDA 95% CI_lower：\n",
    "def auc_LDA_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_LDA_CI_L = auc_LDA_ci_lower(\n",
    "    y_true=y_test, y_score=clf_LDA.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"LDA_AUC_95%CI_lower:\", AUC_LDA_CI_L)\n",
    "\n",
    "# auc_LDA 95% CI_upper：\n",
    "def auc_LDA_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_LDA_CI_U = auc_LDA_ci_upper(\n",
    "    y_true=y_test, y_score=clf_LDA.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"LDA_AUC_95%CI_upper:\", AUC_LDA_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_LDA.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % LDA_Testing_AUC + '(%0.2f' % AUC_LDA_CI_L + '-' + '%0.2f)' % AUC_LDA_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('LDA ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LDA_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA SHAP\n",
    "explainer = shap.LinearExplainer(clf_LDA, x_selected)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"LDA SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'LDA_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_LDA.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='LDA Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"LDA Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LDA_Cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_LDA.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0,1,0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig,ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('LDA Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LDA_DCA.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB tuning\n",
    "\n",
    "param_grid = dict(priors=[[0.68, 0.32], [0.7, 0.3], [0.8, 0.2]], var_smoothing=[1e-9, 1e-8, 1e-7], )\n",
    "model = GaussianNB()\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "print('\\n', \"GNB Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# GNB fit train set\n",
    "clf_GNB = grid.best_estimator_\n",
    "clf_GNB.fit(x_selected, y_train)\n",
    "\n",
    "# GNB AUC in Training set\n",
    "GNB_Training_AUC = roc_auc_score(y_train, clf_GNB.predict_proba(x_selected)[:, 1])\n",
    "print(\"GNB_Training_AUC:\", GNB_Training_AUC)\n",
    "GNB_Training_AUC_CI = auc_ci(y_true=y_train, y_score=clf_GNB.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"GNB_Training_AUC_95%CI:\", GNB_Training_AUC_CI)\n",
    "\n",
    "# GNB confusion_matrix in Training set\n",
    "print(\"GNB模型Training set中混淆矩阵：\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_true = y_train, y_pred = clf_GNB.predict(x_selected)).ravel()\n",
    "print(\"GNB Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# GNB Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"GNB Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"GNB Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# GNB Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"GNB Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples=n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"GNB Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# GNB Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"GNB Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp \n",
    "n_samples= tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"GNB Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# GNB AUC in Testing set\n",
    "GNB_Testing_AUC = roc_auc_score(y_test,clf_GNB.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"GNB AUC in Testing set:\", GNB_Testing_AUC)\n",
    "GNB_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=clf_GNB.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "print(\"GNB AUC in Testing set 95%CI:\", GNB_Testing_AUC_CI)\n",
    "\n",
    "# GNB confusion_matrix in Testing set\n",
    "print(\"GNB confusion_matrix in Testing set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_true = y_test, y_pred = clf_GNB.predict(x_selected_test)).ravel()\n",
    "print(\"GNB Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# GNB Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"GNB Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"GNB Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# GNB Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"GNB Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"GNB Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# GNB Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"GNB Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp \n",
    "n_samples= tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"GNB Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# GNB test ROC\n",
    "\n",
    "# auc_GNB 95% CI_lower：\n",
    "def auc_GNB_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) + (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "AUC_GNB_CI_L = auc_GNB_ci_lower(y_true=y_test, y_score=clf_GNB.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"GNB_AUC_95%CI_lower:\", AUC_GNB_CI_L)\n",
    "\n",
    "# auc_GNB 95% CI_upper：\n",
    "def auc_GNB_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) + (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "AUC_GNB_CI_U = auc_GNB_ci_upper(y_true=y_test, y_score=clf_GNB.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"GNB_AUC_95%CI_upper:\", AUC_GNB_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_GNB.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % GNB_Testing_AUC + '(%0.2f' % AUC_GNB_CI_L + '-' + '%0.2f)' % AUC_GNB_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('GNB ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\GNB_ROC.png', dpi = 300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB SHAP\n",
    "explainer = shap.KernelExplainer(clf_GNB.predict, x_selected)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"GNB SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'GNB_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_GNB.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='GNB Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean predicted value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of positives\", fontsize=12)\n",
    "plt.title(\"GNB Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\GNB_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNB_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_GNB.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('GNB Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\GNB_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART tuning\n",
    "param_grid = {\n",
    "    'max_depth': [i for i in range(1, 10)],\n",
    "    'min_samples_leaf': [i for i in range(1, 6)],\n",
    "    'min_samples_split': [i for i in range(2, 6)],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "}\n",
    "model = DecisionTreeClassifier()\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                    scoring=scoring, cv=kfold)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "print('\\n', \"CART Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# CART fit training set\n",
    "clf_CART = grid.best_estimator_\n",
    "clf_CART.fit(x_selected, y_train)\n",
    "\n",
    "# CART AUC in Training set\n",
    "CART_Training_AUC = roc_auc_score(\n",
    "    y_train, clf_CART.predict_proba(x_selected)[:, 1])\n",
    "print(\"CART_Training_AUC:\", CART_Training_AUC)\n",
    "CART_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=clf_CART.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"CART_Training_AUC_95%CI:\", CART_Training_AUC_CI)\n",
    "\n",
    "# CART confusion_matrix in Training set\n",
    "print(\"CART confusion_matrix in Training set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=clf_CART.predict(x_selected)).ravel()\n",
    "print(\"CART Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# CART Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"CART Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"CART Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# CART Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"CART Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"CART Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# CART Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"CART Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"CART Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# CART AUC in Testing set\n",
    "CART_Testing_AUC = roc_auc_score(\n",
    "    y_test, clf_CART.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"CART_Testing_AUC:\", CART_Testing_AUC)\n",
    "CART_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=clf_CART.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"CART_Testing_AUC_95%CI:\", CART_Testing_AUC_CI)\n",
    "\n",
    "# CART confusion_matrix in Testing set\n",
    "print(\"CART confusion_matrix in Testing set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=clf_CART.predict(x_selected_test)).ravel()\n",
    "print(\"CART Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# CART Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"CART Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"CART Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# CART Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"CART Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"CART Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# CART Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"CART Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"CART Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# CART test ROC\n",
    "\n",
    "# auc_CART 95% CI_lower：\n",
    "def auc_CART_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "AUC_CART_CI_L = auc_CART_ci_lower(\n",
    "    y_true=y_test, y_score=clf_CART.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"CART_AUC_95%CI_lower:\", AUC_CART_CI_L)\n",
    "\n",
    "# auc_CART 95% CI_upper：\n",
    "def auc_CART_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "AUC_CART_CI_U = auc_CART_ci_upper(\n",
    "    y_true=y_test, y_score=clf_CART.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"CART_AUC_95%CI_upper:\", AUC_CART_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_CART.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % CART_Testing_AUC + '(%0.2f' % AUC_CART_CI_L + '-' + '%0.2f)' % AUC_CART_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('CART ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\CART_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART SHAP\n",
    "explainer = shap.TreeExplainer(clf_CART)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"bar\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"CART SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'CART_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cart calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_CART.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='CART Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean predicted value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of positives\", fontsize=12)\n",
    "plt.title(\"CART Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\CART_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_CART.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('CART Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\CART_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model pairwise comprision with FDR\n",
    "\n",
    "def ROC_DelongTest(clf_names, clf_methods, x_test, y_label):\n",
    "    results = []\n",
    "\n",
    "    # p value\n",
    "    for n, clf1 in enumerate(clf_methods):\n",
    "        for i, clf2 in enumerate(clf_methods[n + 1:], start=n + 1):\n",
    "            y_predict_1 = clf1.predict_proba(x_test)[:, 1]\n",
    "            y_predict_2 = clf2.predict_proba(x_test)[:, 1]\n",
    "            p_value = DelongTest(y_predict_1, y_predict_2, y_label).p_value\n",
    "            auc1 = roc_auc_score(y_label, y_predict_1)\n",
    "            auc2 = roc_auc_score(y_label, y_predict_2)\n",
    "            results.append([clf_names[n], clf_names[i], auc1, auc2, p_value])\n",
    "\n",
    "    # FDR\n",
    "    p_values = [result[-1] for result in results]\n",
    "    _, corrected_p_values, _, _ = multipletests(\n",
    "        p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "    # p value after FDR\n",
    "    for i in range(len(results)):\n",
    "        results[i][-1] = corrected_p_values[i]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "models = {'LR': lr, 'KNN': clf_KNN, 'SVM': clf_SVM,\n",
    "          'LDA': clf_LDA, 'GNB': clf_GNB, 'CART': clf_CART}\n",
    "\n",
    "ROC_results = ROC_DelongTest(list(models.keys()), list(\n",
    "    models.values()), x_selected_test, y_test)\n",
    "\n",
    "df = pd.DataFrame(ROC_results, columns=[\n",
    "                  'Model 1', 'Model 2', 'AUC 1', 'AUC 2', 'Adjusted p-value'])\n",
    "df.to_excel(r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\all_single_ROC.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of single model pairwise comprision with FDR\n",
    "\n",
    "df_pvalues = pd.read_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\all_single_ROC.xlsx')\n",
    "clf_names = ['LR', 'KNN', 'SVM', 'LDA', 'GNB', 'CART']\n",
    "num_models = len(clf_names)\n",
    "p_value_matrix = np.zeros((num_models, num_models))\n",
    "\n",
    "for index, row in df_pvalues.iterrows():\n",
    "    i = clf_names.index(row['Model 1'])\n",
    "    j = clf_names.index(row['Model 2'])\n",
    "    p_value_matrix[i, j] = row['Adjusted p-value']\n",
    "    p_value_matrix[j, i] = row['Adjusted p-value']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.cm.YlGnBu\n",
    "heatmap = ax.imshow(p_value_matrix, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "for i in range(len(clf_names)):\n",
    "    for j in range(len(clf_names)):\n",
    "        if i != j:\n",
    "            p_val = p_value_matrix[i, j]\n",
    "            if p_val < 0.001:\n",
    "                display_text = \"<0.001\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            elif p_val < 0.05:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            else:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'black'\n",
    "                weight = 'normal'\n",
    "            text = ax.text(j, i, display_text,\n",
    "                           ha=\"center\", va=\"center\", color=color, fontweight=weight)\n",
    "\n",
    "ax.set_xticks(np.arange(num_models))\n",
    "ax.set_yticks(np.arange(num_models))\n",
    "ax.set_xticklabels(clf_names, fontsize=12)\n",
    "ax.set_yticklabels(clf_names, fontsize=12)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "ax.set_title(\n",
    "    'Adjusted p-values from Delong Test with FDR Correction', fontsize=15)\n",
    "\n",
    "cbar = fig.colorbar(heatmap, ax=ax, shrink=0.6, aspect=30)\n",
    "cbar.ax.set_ylabel('Adjusted p-value', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\all_single_heatmap_FDR_corrected.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRI and single models pairwise comprision with FDR\n",
    "\n",
    "models = {'LR': lr, 'KNN': clf_KNN, 'SVM': clf_SVM, 'LDA': clf_LDA,\n",
    "          'GNB': clf_GNB, 'CART': clf_CART, 'clf_MRI': clf_MRI}\n",
    "\n",
    "aucs = {}\n",
    "for name, model in models.items():\n",
    "    if name == 'clf_MRI':\n",
    "        predict = model.predict(x_test_step1[independent_vars])\n",
    "    else:\n",
    "        predict = model.predict_proba(x_selected_test)[:, 1]\n",
    "    aucs[name] = roc_auc_score(y_test, predict)\n",
    "\n",
    "p_values = []\n",
    "comparisons = []\n",
    "for i, (name_1, auc_1) in enumerate(aucs.items()):\n",
    "    for j in range(i+1, len(aucs)):\n",
    "        name_2, auc_2 = list(aucs.items())[j]\n",
    "        if name_1 == 'clf_MRI':\n",
    "            predict_1 = models[name_1].predict(x_test_step1[independent_vars])\n",
    "        else:\n",
    "            predict_1 = models[name_1].predict_proba(x_selected_test)[:, 1]\n",
    "        if name_2 == 'clf_MRI':\n",
    "            predict_2 = models[name_2].predict(x_test_step1[independent_vars])\n",
    "        else:\n",
    "            predict_2 = models[name_2].predict_proba(x_selected_test)[:, 1]\n",
    "        pvalue = DelongTest(predict_1, predict_2, y_test).p_value\n",
    "        p_values.append(pvalue)\n",
    "        comparisons.append((name_1, name_2, auc_1, auc_2, pvalue))\n",
    "\n",
    "rejected, p_values_corrected, _, _ = multipletests(\n",
    "    p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "for i in range(len(comparisons)):\n",
    "    comparisons[i] = comparisons[i][:4] + (p_values_corrected[i],)\n",
    "\n",
    "df = pd.DataFrame(comparisons, columns=[\n",
    "                  'Model 1', 'Model 2', 'AUC 1', 'AUC 2', 'Adjusted p-value'])\n",
    "df.to_excel('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\Single_MRI_ROC.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of MRI and single models pairwise comprision with FDR\n",
    "df_pvalues = pd.read_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\Single_MRI_ROC.xlsx')\n",
    "\n",
    "df_pvalues.replace('clf_MRI', 'Logit', inplace=True)\n",
    "clf_names = ['Logit', 'LR', 'KNN', 'SVM', 'LDA', 'GNB', 'CART']\n",
    "num_models = len(clf_names)\n",
    "p_value_matrix = np.zeros((num_models, num_models))\n",
    "\n",
    "for index, row in df_pvalues.iterrows():\n",
    "    i = clf_names.index(row['Model 1'])\n",
    "    j = clf_names.index(row['Model 2'])\n",
    "    p_value_matrix[i, j] = row['Adjusted p-value']\n",
    "    p_value_matrix[j, i] = row['Adjusted p-value']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.cm.YlGnBu\n",
    "heatmap = ax.imshow(p_value_matrix, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "for i in range(len(clf_names)):\n",
    "    for j in range(len(clf_names)):\n",
    "        if i != j:\n",
    "            p_val = p_value_matrix[i, j]\n",
    "            if p_val < 0.001:\n",
    "                display_text = \"<0.001\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            elif p_val < 0.05:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            else:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'black'\n",
    "                weight = 'normal'\n",
    "            text = ax.text(j, i, display_text,\n",
    "                           ha=\"center\", va=\"center\", color=color, fontweight=weight)\n",
    "\n",
    "ax.set_xticks(np.arange(num_models))\n",
    "ax.set_yticks(np.arange(num_models))\n",
    "ax.set_xticklabels(clf_names, fontsize=12)\n",
    "ax.set_yticklabels(clf_names, fontsize=12)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "ax.set_title(\n",
    "    'Adjusted p-values from Delong Test with FDR Correction', fontsize=15)\n",
    "\n",
    "cbar = fig.colorbar(heatmap, ax=ax, shrink=0.6, aspect=30)\n",
    "cbar.ax.set_ylabel('Adjusted p-value', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\single_MRI_heatmap_FDR_corrected.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of MRI and single models \n",
    "\n",
    "aucs = {'Logit': clf_MRI_test_auc, 'LR': LR_Testing_AUC, 'KNN': KNN_Testing_AUC, 'SVM': SVM_Testing_AUC, 'LDA': LDA_Testing_AUC, 'GNB': GNB_Testing_AUC, 'CART': CART_Testing_AUC}\n",
    "conf_intervals = { 'Logit': clf_MRI_test_auc_CI,'LR': LR_Testing_AUC_CI, 'KNN': KNN_Testing_AUC_CI, 'SVM': SVM_Testing_AUC_CI, 'LDA': LDA_Testing_AUC_CI, 'GNB': GNB_Testing_AUC_CI, 'CART': CART_Testing_AUC_CI}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(\n",
    "    aucs.keys(),\n",
    "    aucs.values(),\n",
    "    color=['red', 'green', 'blue', 'orange', 'purple', 'gray', 'pink'],\n",
    "    yerr=[(conf_intervals[m][1] - conf_intervals[m][0]) / 2 for m in aucs.keys()],\n",
    "    capsize=5,\n",
    ")\n",
    "ax.set_title('AUC Comparisons', fontsize=15)\n",
    "ax.set_xticklabels(aucs.keys(), rotation=45, ha='right', fontsize=12)\n",
    "ax.set_ylabel('AUC', fontsize=12)\n",
    "plt.title('Logit and Single Models', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Logit_single_C.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRI and IDI：single models vs MRI\n",
    "\n",
    "y_pred_MRI = clf_MRI.predict(x_test_step1[independent_vars])\n",
    "df_MRI = pd.DataFrame(y_pred_MRI, columns=['MRI_pred']).reset_index(drop=True)\n",
    "y_true = y_test\n",
    "y_true = y_test.reset_index(drop=True)\n",
    "\n",
    "# LR\n",
    "y_pred_lr = lr.predict_proba(x_selected_test)[:, 1]\n",
    "df_lr = pd.DataFrame(y_pred_lr, columns=['lr_pred']).reset_index(drop=True)\n",
    "df_MRI_lr = pd.concat([df_MRI, df_lr, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and LR:', '\\n')\n",
    "lr_nri = nri(df_MRI_lr, 'MRI_pred', 'lr_pred', 'label')\n",
    "lr_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_LR.xlsx\", index=False)\n",
    "\n",
    "# KNN\n",
    "y_pred_KNN = clf_KNN.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_KNN = np.where(y_pred_KNN == 0, 0.000001, y_pred_KNN)\n",
    "y_pred_KNN = np.where(y_pred_KNN == 1, 0.999999, y_pred_KNN)\n",
    "df_KNN = pd.DataFrame(y_pred_KNN, columns=['KNN_pred']).reset_index(drop=True)\n",
    "df_MRI_KNN = pd.concat([df_MRI, df_KNN, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and KNN:', '\\n')\n",
    "KNN_nri = nri(df_MRI_KNN, 'MRI_pred', 'KNN_pred', 'label')\n",
    "KNN_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_KNN.xlsx\", index=False)\n",
    "\n",
    "# SVM\n",
    "y_pred_SVM = clf_SVM.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_SVM = np.where(y_pred_SVM == 0, 0.000001, y_pred_SVM)\n",
    "y_pred_SVM = np.where(y_pred_SVM == 1, 0.999999, y_pred_SVM)\n",
    "df_SVM = pd.DataFrame(y_pred_SVM, columns=['SVM_pred']).reset_index(drop=True)\n",
    "df_MRI_SVM = pd.concat([df_MRI, df_SVM, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and SVM:', '\\n')\n",
    "SVM_nri = nri(df_MRI_SVM, 'MRI_pred', 'SVM_pred', 'label')\n",
    "SVM_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_SVM.xlsx\", index=False)\n",
    "\n",
    "# LDA\n",
    "y_pred_LDA = clf_LDA.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_LDA = np.where(y_pred_LDA == 0, 0.000001, y_pred_LDA)\n",
    "y_pred_LDA = np.where(y_pred_LDA == 1, 0.999999, y_pred_LDA)\n",
    "df_LDA = pd.DataFrame(y_pred_LDA, columns=['LDA_pred']).reset_index(drop=True)\n",
    "df_MRI_LDA = pd.concat([df_MRI, df_LDA, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and LDA:', '\\n')\n",
    "LDA_nri = nri(df_MRI_LDA, 'MRI_pred', 'LDA_pred', 'label')\n",
    "LDA_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_LDA.xlsx\", index=False)\n",
    "\n",
    "# GNB\n",
    "y_pred_GNB = clf_GNB.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_GNB = np.where(y_pred_GNB == 0, 0.000001, y_pred_GNB)\n",
    "y_pred_GNB = np.where(y_pred_GNB == 1, 0.999999, y_pred_GNB)\n",
    "df_GNB = pd.DataFrame(y_pred_GNB, columns=['GNB_pred']).reset_index(drop=True)\n",
    "df_MRI_GNB = pd.concat([df_MRI, df_GNB, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and GNB:', '\\n')\n",
    "GNB_nri = nri(df_MRI_GNB, 'MRI_pred', 'GNB_pred', 'label')\n",
    "GNB_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_GNB.xlsx\", index=False)\n",
    "\n",
    "# CART\n",
    "y_pred_CART = clf_CART.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_CART = np.where(y_pred_CART == 0, 0.000001, y_pred_CART)\n",
    "y_pred_CART = np.where(y_pred_CART == 1, 0.999999, y_pred_CART)\n",
    "df_CART = pd.DataFrame(y_pred_CART, columns=[\n",
    "                       'CART_pred']).reset_index(drop=True)\n",
    "df_MRI_CART = pd.concat([df_MRI, df_CART, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and CART:', '\\n')\n",
    "CART_nri = nri(df_MRI_CART, 'MRI_pred', 'CART_pred', 'label')\n",
    "CART_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_CART.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF tuning\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 50],\n",
    "    'max_depth': [2, 5, 10, 20, 30],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "model = RandomForestClassifier(criterion='gini')\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                    scoring=scoring, cv=kfold)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "print('\\n', \"RF Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# RF fit train set\n",
    "clf_RF = grid.best_estimator_\n",
    "clf_RF.fit(x_selected, y_train)\n",
    "\n",
    "# RF AUC in Training set\n",
    "RF_Training_AUC = roc_auc_score(\n",
    "    y_train, clf_RF.predict_proba(x_selected)[:, 1])\n",
    "print(\"RF_Training_AUC:\", RF_Training_AUC)\n",
    "RF_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=clf_RF.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"RF_Training_AUC_95%CI:\", RF_Training_AUC_CI)\n",
    "\n",
    "# RF confusion_matrix in Training set\n",
    "print(\"RF confusion_matrix in Training set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=clf_RF.predict(x_selected)).ravel()\n",
    "print(\"RF Training set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# RF Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"RF Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"RF Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# RF Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"RF Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"RF Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# RF Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"RF Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"RF Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# RF AUC in Testing set\n",
    "RF_Testing_AUC = roc_auc_score(\n",
    "    y_test, clf_RF.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"RF_Testing_AUC:\", RF_Testing_AUC)\n",
    "RF_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=clf_RF.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"RF_Testing_AUC_95%CI:\", RF_Testing_AUC_CI)\n",
    "\n",
    "# RF confusion_matrix in Testing set\n",
    "print(\"RF confusion_matrix in Testing set：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=clf_RF.predict(x_selected_test)).ravel()\n",
    "print(\"RF Testing set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# RF Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"RF Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"RF Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# RF Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"RF Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"RF Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# RF Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"RF Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"RF Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# RF test ROC\n",
    "\n",
    "# auc_RF 95% CI_lower：\n",
    "def auc_RF_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_RF_CI_L = auc_RF_ci_lower(\n",
    "    y_true=y_test, y_score=clf_RF.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"RF_AUC_95%CI_lower:\", AUC_RF_CI_L)\n",
    "\n",
    "# auc_RF 95% CI_upper：\n",
    "def auc_RF_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_RF_CI_U = auc_RF_ci_upper(\n",
    "    y_true=y_test, y_score=clf_RF.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"RF_AUC_95%CI_upper:\", AUC_RF_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_RF.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % RF_Testing_AUC + '(%0.2f' % AUC_RF_CI_L + '-' + '%0.2f)' % AUC_RF_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('RF ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\RF_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF SHAP\n",
    "explainer = shap.TreeExplainer(clf_RF)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"bar\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"RF SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'RF_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_RF.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='RF Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"RF Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\RF_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_RF.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('RF Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\RF_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging tuning\n",
    "\n",
    "param_grid = {'n_estimators': [10, 50, 100, 200, 500],\n",
    "              'max_features': [1.0, 2.0, 3.0, 5.0],\n",
    "              'random_state': [5, 10, 30]\n",
    "              }\n",
    "\n",
    "model = BaggingClassifier(base_estimator=clf_CART, n_jobs=1)\n",
    "\n",
    "grid = GridSearchCV(estimator=model,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    cv=10)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "\n",
    "print('\\n', \"bag Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# Bagging fit training set\n",
    "bag_best_parameters = grid.best_estimator_\n",
    "bag = grid.best_estimator_\n",
    "bag.fit(x_selected, y_train)\n",
    "\n",
    "# bag_Training_AUC\n",
    "bag_Training_AUC = roc_auc_score(y_train, bag.predict_proba(x_selected)[:, 1])\n",
    "print(\"bag_Training_AUC:\", bag_Training_AUC)\n",
    "bag_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=bag.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"bag_Training_AUC_95%CI:\", bag_Training_AUC_CI)\n",
    "\n",
    "# bag Training set Confusion matrix\n",
    "print(\"bag Training set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=bag.predict(x_selected)).ravel()\n",
    "print(\"bag Training set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# bag Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"bag Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"bag Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# bag Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"bag Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"bag Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# bag Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"bag Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"bag Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# bag_Testing_AUC\n",
    "bag_Testing_AUC = roc_auc_score(\n",
    "    y_test, bag.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"bag_Testing_AUC:\", bag_Testing_AUC)\n",
    "bag_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=bag.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"bag_Testing_AUC_95%CI:\", bag_Testing_AUC_CI)\n",
    "\n",
    "# bag Testing set Confusion matrix\n",
    "print(\"bag Testing set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=bag.predict(x_selected_test)).ravel()\n",
    "print(\"bag Testing set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# bag Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"bag Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"bag Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# bag Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"bag Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"bag Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# bag Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"bag Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"bag Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# bag test ROC\n",
    "\n",
    "# auc_bag 95% CI_lower：\n",
    "def auc_bag_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_bag_CI_L = auc_bag_ci_lower(\n",
    "    y_true=y_test, y_score=bag.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"bag_AUC_95%CI_lower:\", AUC_bag_CI_L)\n",
    "\n",
    "# auc_bag 95% CI_upper：\n",
    "def auc_bag_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC ** 2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC ** 2) +\n",
    "                  (N2 - 1) * (Q2 - AUC ** 2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_bag_CI_U = auc_bag_ci_upper(\n",
    "    y_true=y_test, y_score=bag.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "# print(\"bag_AUC_95%CI_upper:\", AUC_bag_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = bag.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1,\n",
    "         label='AUC = %0.2f' % bag_Testing_AUC + '(%0.2f' % AUC_bag_CI_L + '-' + '%0.2f)' % AUC_bag_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Bagging ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Bag_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging SHAP\n",
    "best_bag_model = grid.best_estimator_\n",
    "shap_values_list = []\n",
    "\n",
    "for estimator in best_bag_model.estimators_:\n",
    "    explainer = shap.TreeExplainer(estimator)\n",
    "    shap_values = explainer.shap_values(x_selected_test)\n",
    "    if len(shap_values) == 2:\n",
    "        shap_values = shap_values[1]\n",
    "    shap_values_list.append(shap_values)\n",
    "\n",
    "average_shap_values = np.mean(np.array(shap_values_list), axis=0)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(average_shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"Bag SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'Bag_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, bag.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='Bagging Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=15)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=15)\n",
    "plt.title(\"Bagging Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Bag_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = bag.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('Bagging Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Bag_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost tuning\n",
    "param_grid = {'learning_rate': [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 10],\n",
    "              'n_estimators': [10, 100, 200, 300, 400],\n",
    "\n",
    "              }\n",
    "model = AdaBoostClassifier(base_estimator=clf_CART, algorithm=\"SAMME.R\")\n",
    "\n",
    "grid = GridSearchCV(estimator=model,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    cv=10)\n",
    "grid.fit(x_train_sc, y_train)\n",
    "\n",
    "print('\\n', \"bag Best: %f using: %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# AdaBoost fit training set\n",
    "ada_best_parameters = grid.best_estimator_\n",
    "ada = grid.best_estimator_\n",
    "ada.fit(x_selected, y_train)\n",
    "\n",
    "# AdaBoost_Training_AUC\n",
    "AdaBoost_Training_AUC = roc_auc_score(\n",
    "    y_train, ada.predict_proba(x_selected)[:, 1])\n",
    "print(\"AdaBoost_Training_AUC:\", AdaBoost_Training_AUC)\n",
    "AdaBoost_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=ada.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"AdaBoost_Training_AUC_95%CI:\", AdaBoost_Training_AUC_CI)\n",
    "\n",
    "# AdaBoost Training set Confusion matrix\n",
    "print(\"AdaBoost Training set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=ada.predict(x_selected)).ravel()\n",
    "print(\"AdaBoost Training set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# AdaBoost Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"AdaBoost Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"AdaBoost Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# AdaBoost Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"AdaBoost Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"AdaBoost Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# AdaBoost Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"AdaBoost Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"AdaBoost Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# AdaBoost_Testing_AUC\n",
    "AdaBoost_Testing_AUC = roc_auc_score(\n",
    "    y_test, ada.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"AdaBoost_Testing_AUC:\", AdaBoost_Testing_AUC)\n",
    "AdaBoost_Testing_AUC_CI = auc_ci(\n",
    "    y_true=y_test, y_score=ada.predict_proba(x_selected_test)[:, 1], positive=1)\n",
    "print(\"AdaBoost_Testing_AUC_95%CI:\", AdaBoost_Testing_AUC_CI)\n",
    "\n",
    "# AdaBoost Testing set Confusion matrix\n",
    "print(\"AdaBoost Testing set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=ada.predict(x_selected_test)).ravel()\n",
    "print(\"AdaBoost Testing set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# AdaBoost Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"AdaBoost Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"AdaBoost Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# AdaBoost Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"AdaBoost Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"AdaBoost Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# AdaBoost Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"AdaBoost Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"AdaBoost Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# AdaBoost test set ROC\n",
    "\n",
    "# auc_ada 95% CI_lower：\n",
    "def auc_ada_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_ada_CI_L = auc_ada_ci_lower(y_true=y_test,\n",
    "                                y_score=ada.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"ada_AUC_95%CI_lower:\", AUC_ada_CI_L)\n",
    "\n",
    "# auc_ada 95% CI_upper：\n",
    "def auc_ada_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_ada_CI_U = auc_ada_ci_upper(y_true=y_test,\n",
    "                                y_score=ada.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"ada_AUC_95%CI_upper:\", AUC_ada_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = ada.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr,\n",
    "         tpr,\n",
    "         color='darkorange',\n",
    "         lw=1,\n",
    "         label='AUC = %0.2f' % AdaBoost_Testing_AUC + '(%0.2f' % AUC_ada_CI_L + '-' +\n",
    "         '%0.2f)' % AUC_ada_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('AdaBoost ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\AdaBoost_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost SHAP\n",
    "best_bag_model = grid.best_estimator_\n",
    "shap_values_list = []\n",
    "\n",
    "for estimator in best_bag_model.estimators_:\n",
    "    explainer = shap.TreeExplainer(estimator)\n",
    "    shap_values = explainer.shap_values(x_selected_test)\n",
    "    if len(shap_values) == 2:\n",
    "        shap_values = shap_values[1]\n",
    "    shap_values_list.append(shap_values)\n",
    "\n",
    "average_shap_values = np.mean(np.array(shap_values_list), axis=0)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(average_shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"AdaBoost SHAP Value\")\n",
    "plt.yticks(fontsize=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'AdaBoost_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, ada.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='AdaBoost Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"AdaBoost Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\AdaBoost_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = ada.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('AdaBoost Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\AdaBoost_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT parameters：The parameters 'n_estimators' and 'learning_rate' are set using the tuned parameters from AdaBoost, while the remaining parameters are configured using the tuned parameters from CART and RF.\n",
    "\n",
    "clf_GBDT = GBDT(loss='deviance',\n",
    "                n_estimators=400,\n",
    "                learning_rate=1,\n",
    "                min_samples_split=8,\n",
    "                min_samples_leaf=2,\n",
    "                max_depth=4,\n",
    "                max_features='sqrt',\n",
    "                subsample=0.8,\n",
    "                random_state=None)\n",
    "\n",
    "# GBDT fit training set\n",
    "clf_GBDT.fit(x_selected, y_train)\n",
    "\n",
    "# GBDT_Training_AUC\n",
    "GBDT_Training_AUC = roc_auc_score(\n",
    "    y_train, clf_GBDT.predict_proba(x_selected)[:, 1])\n",
    "print(\"GBDT_Training_AUC:\", GBDT_Training_AUC)\n",
    "GBDT_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=clf_GBDT.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"GBDT_Training_AUC_95%CI:\", GBDT_Training_AUC_CI)\n",
    "\n",
    "# GBDT Training set Confusion matrix\n",
    "print(\"GBDT Training set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=clf_GBDT.predict(x_selected)).ravel()\n",
    "print(\"GBDT Training set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# GBDT Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"GBDT Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"GBDT Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# GBDT Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"GBDT Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"GBDT Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# GBDT Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"GBDT Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"GBDT Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# GBDT_Testing_AUC\n",
    "GBDT_Testing_AUC = roc_auc_score(\n",
    "    y_test, clf_GBDT.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"GBDT_Testing_AUC:\", GBDT_Testing_AUC)\n",
    "GBDT_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=clf_GBDT.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"GBDT_Testing_AUC_95%CI:\", GBDT_Testing_AUC_CI)\n",
    "\n",
    "# GBDT Testing set Confusion matrix\n",
    "print(\"GBDT Testing set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=clf_GBDT.predict(x_selected_test)).ravel()\n",
    "print(\"GBDT Testing set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# GBDT Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"GBDT Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"GBDT Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# GBDT Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"GBDT Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"GBDT Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# GBDT Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"GBDT Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"GBDT Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# GBDT test ROC\n",
    "\n",
    "# auc_GBDT 95% CI_lower：\n",
    "def auc_GBDT_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_GBDT_CI_L = auc_GBDT_ci_lower(\n",
    "    y_true=y_test,\n",
    "    y_score=clf_GBDT.predict_proba(x_selected_test)[:, 1],\n",
    "    positive=1)\n",
    "# print(\"GBDT_AUC_95%CI_lower:\", AUC_GBDT_CI_L)\n",
    "\n",
    "# auc_GBDT 95% CI_upper：\n",
    "def auc_GBDT_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_GBDT_CI_U = auc_GBDT_ci_upper(\n",
    "    y_true=y_test,\n",
    "    y_score=clf_GBDT.predict_proba(x_selected_test)[:, 1],\n",
    "    positive=1)\n",
    "# print(\"GBDT_AUC_95%CI_upper:\", AUC_GBDT_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = clf_GBDT.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr,\n",
    "         tpr,\n",
    "         color='darkorange',\n",
    "         lw=1,\n",
    "         label='AUC = %0.2f' % GBDT_Testing_AUC + '(%0.2f' % AUC_GBDT_CI_L + '-' +\n",
    "         '%0.2f)' % AUC_GBDT_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('GBDT ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\GBDT_ROC.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT SHAP\n",
    "explainer = shap.TreeExplainer(clf_GBDT)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"GBDT SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'GBDT_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, clf_GBDT.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='GBDT Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"GBDT Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\GBDT_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = clf_GBDT.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('GBDT Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\GBDT_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost tuning with shap-hypetune\n",
    "\n",
    "param_dist_hyperopt = {\n",
    "    'n_estimators':hp.randint('n_estimators',500),\n",
    "    'max_depth': 15 + hp.randint('num_leaves', 5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}\n",
    "clf_xgb = XGBClassifier(random_state=0, verbosity=0, n_jobs=-1)\n",
    "model = BoostSearch(clf_xgb,\n",
    "                    param_grid=param_dist_hyperopt,\n",
    "                    n_iter=8,\n",
    "                    sampling_seed=0)\n",
    "model.fit(x_train_sc, y_train,\n",
    "          trials=Trials(),\n",
    "          eval_set=[(x_selected, y_train)],\n",
    "          early_stopping_rounds=6,\n",
    "          verbose=0)\n",
    "model.estimator_, model.best_params_, model.best_score_\n",
    "print('\\n','shap-hypetune xgb：', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The XGB model employs the tuned parameters derived from the aforementioned SHAP-hypertune process.\n",
    "\n",
    "XGB = XGBClassifier(n_estimators=279,\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=16,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0.0,\n",
    "                    subsample=0.6,\n",
    "                    colsample_bytree=0.8,\n",
    "                    objective='binary:logistic',\n",
    "                    scale_pos_weight=1,\n",
    "                    seed=27)\n",
    "XGB = XGB.fit(x_selected, y_train)\n",
    "\n",
    "# XGB_Training_AUC\n",
    "XGB_Training_AUC = roc_auc_score(y_train, XGB.predict_proba(x_selected)[:, 1])\n",
    "print(\"XGB_Training_AUC:\", XGB_Training_AUC)\n",
    "XGB_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=XGB.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"XGB_Training_AUC_95%CI:\", XGB_Training_AUC_CI)\n",
    "\n",
    "# XGB Training set Confusion matrix\n",
    "print(\"XGB Training set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=XGB.predict(x_selected)).ravel()\n",
    "print(\"XGB Training set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# XGB Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"XGB Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"XGB Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# XGB Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"XGB Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"XGB Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# XGB Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"XGB Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"XGB Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "print(\" \")\n",
    "\n",
    "# XGB_Testing_AUC\n",
    "XGB_Testing_AUC = roc_auc_score(\n",
    "    y_test, XGB.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"XGB_Testing_AUC:\", XGB_Testing_AUC)\n",
    "XGB_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=XGB.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"XGB_Testing_AUC_95%CI:\", XGB_Testing_AUC_CI)\n",
    "\n",
    "# XGB Testing set Confusion matrix\n",
    "print(\"XGB Testing set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=XGB.predict(x_selected_test)).ravel()\n",
    "print(\"XGB Testing set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# XGB Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"XGB Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"XGB Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# XGB Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"XGB Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"XGB Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# XGB Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"XGB Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"XGB Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# XGB test set ROC\n",
    "\n",
    "# auc_XGB 95% CI_lower：\n",
    "def auc_XGB_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_XGB_CI_L = auc_XGB_ci_lower(y_true=y_test,\n",
    "                                y_score=XGB.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"调参后XGB_AUC_95%CI_lower:\", AUC_XGB_CI_L)\n",
    "\n",
    "\n",
    "# auc_XGB 95% CI_upper：\n",
    "def auc_XGB_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_XGB_CI_U = auc_XGB_ci_upper(y_true=y_test,\n",
    "                                y_score=XGB.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"调参后XGB_AUC_95%CI_upper:\", AUC_XGB_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = XGB.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr,\n",
    "         tpr,\n",
    "         color='darkorange',\n",
    "         lw=1,\n",
    "         label='AUC = %0.2f' % XGB_Testing_AUC + '(%0.2f' % AUC_XGB_CI_L + '-' +\n",
    "         '%0.2f)' % AUC_XGB_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('XGBoost ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\XGBoost_ROC.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB SHAP\n",
    "explainer = shap.TreeExplainer(XGB)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"XGB SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'XGB_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost calibration_curve\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, XGB.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='XGBoost Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"XGBoost Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\XGBoost_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = XGB.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('XGBoost Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\XGBoost_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM tuning with shap-hypetune\n",
    "\n",
    "param_dist_hyperopt = {\n",
    "    'n_estimators': hp.randint('n_estimators', 500),\n",
    "    'max_depth': 15 + hp.randint('num_leaves', 5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
    "}\n",
    "\n",
    "clf_lgbm = LGBMClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "model = BoostSearch(clf_lgbm, param_grid=param_dist_hyperopt,\n",
    "                    n_iter=8, sampling_seed=0)\n",
    "model.fit(x_train_sc, y_train, trials=Trials(), eval_set=[\n",
    "          (x_selected, y_train)], early_stopping_rounds=6, verbose=0)\n",
    "model.estimator_, model.best_params_, model.best_score_\n",
    "print('\\n', 'shap-hypetune ightGBM', model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LightGBM model employs the tuned parameters derived from the aforementioned SHAP-hypertune process.\n",
    "\n",
    "lgb = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    max_depth=19,\n",
    "    learning_rate=0.011,\n",
    "    n_estimators=357,\n",
    "    subsample_for_bin=200000,\n",
    "    objective=None,\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,\n",
    "    min_child_weight=0.001,\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,\n",
    "    subsample_freq=0,\n",
    "    colsample_bytree=0.74,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    silent=True,)\n",
    "lgb = lgb.fit(x_selected, y_train)\n",
    "\n",
    "# lgb_Training_AUC\n",
    "lgb_Training_AUC = roc_auc_score(y_train, lgb.predict_proba(x_selected)[:, 1])\n",
    "print(\"lgb_Training_AUC:\", lgb_Training_AUC)\n",
    "lgb_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=lgb.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"lgb_Training_AUC_95%CI:\", lgb_Training_AUC_CI)\n",
    "\n",
    "# lgb Training set Confusion matrix\n",
    "print(\"lgb Training set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=lgb.predict(x_selected)).ravel()\n",
    "print(\"lgb Training set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# lgb Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"lgb Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"lgb Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# lgb Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"lgb Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"lgb Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# lgb Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"lgb Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"lgb Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# lgb_Testing_AUC\n",
    "lgb_Testing_AUC = roc_auc_score(\n",
    "    y_test, lgb.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"lgb_Testing_AUC:\", lgb_Testing_AUC)\n",
    "lgb_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=lgb.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"lgb_Testing_AUC_95%CI:\", lgb_Testing_AUC_CI)\n",
    "\n",
    "# lgb Testing set Confusion matrix\n",
    "print(\"lgb Testing set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=lgb.predict(x_selected_test)).ravel()\n",
    "print(\"lgb Testing set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# lgb Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"lgb Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"lgb Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# lgb Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"lgb Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"lgb Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# lgb Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"lgb Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"lgb Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# lgb Testing set ROC\n",
    "\n",
    "# auc_lgb 95% CI_lower：\n",
    "def auc_lgb_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_lgb_CI_L = auc_lgb_ci_lower(y_true=y_test,\n",
    "                                y_score=lgb.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"调参后lgb_AUC_95%CI_lower:\", AUC_lgb_CI_L)\n",
    "\n",
    "# auc_lgb 95% CI_upper：\n",
    "def auc_lgb_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_lgb_CI_U = auc_lgb_ci_upper(y_true=y_test,\n",
    "                                y_score=lgb.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"调参后lgb_AUC_95%CI_upper:\", AUC_lgb_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = lgb.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr,\n",
    "         tpr,\n",
    "         color='darkorange',\n",
    "         lw=1,\n",
    "         label='AUC = %0.2f' % lgb_Testing_AUC + '(%0.2f' % AUC_lgb_CI_L + '-' +\n",
    "         '%0.2f)' % AUC_lgb_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('LightGBM ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LightGBM_ROC.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM SHAP\n",
    "explainer = shap.TreeExplainer(lgb)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"bar\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"LGB SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'LBG_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM calibration_curve\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, lgb.predict_proba(x_selected_test)[:,1], n_bins = 4\n",
    "    #                   , strategy = 'quantile'\n",
    "                     )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"rs-\", label='LightGBM Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"LightGBM Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LightGBM_cal.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightGBM_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = lgb.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('LightGBM Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\LightGBM_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost tuning with CatboostTuner\n",
    "import catboost as Catboost\n",
    "import CatboostTuner\n",
    "\n",
    "tuned_catboost = CatboostTuner.CatboostTuner()\n",
    "tuned_catboost.run(X=x_train_sc, y=y_train, nfold=5)\n",
    "\n",
    "predictions = tuned_catboost.predict(x_test_sc)\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, predictions))\n",
    "print(tuned_catboost.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The Catboost model employs the tuned parameters derived from the aforementioned CatboostTuner process.\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat = CatBoostClassifier(\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    iterations=1100,\n",
    "    learning_rate=0.03,\n",
    "    bootstrap_type='Bernoulli',\n",
    "    subsample=0.4,\n",
    "    random_strength=1,\n",
    "    rsm=0.1,\n",
    "    max_depth=3,\n",
    "    grow_policy='SymmetricTree',\n",
    "    l2_leaf_reg=1,\n",
    ")\n",
    "cat = cat.fit(x_selected, y_train)\n",
    "\n",
    "# cat_Training_AUC\n",
    "cat_Training_AUC = roc_auc_score(y_train, cat.predict_proba(x_selected)[:, 1])\n",
    "print(\"cat_Training_AUC:\", cat_Training_AUC)\n",
    "cat_Training_AUC_CI = auc_ci(\n",
    "    y_true=y_train, y_score=cat.predict_proba(x_selected)[:, 1], positive=1)\n",
    "print(\"cat_Training_AUC_95%CI:\", cat_Training_AUC_CI)\n",
    "\n",
    "# cat Training set Confusion matrix\n",
    "print(\"cat Training set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_train, y_pred=cat.predict(x_selected)).ravel()\n",
    "print(\"cat Training set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "\n",
    "# cat Training set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"cat Training set Sensitivity:\", sensitivity)\n",
    "n_samples_sen = fn + tp\n",
    "n_samples = n_samples_sen\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"cat Training set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# cat Training set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"cat Training set Specificity:\", specificity)\n",
    "n_samples_spe = tn + fp\n",
    "n_samples = n_samples_spe\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"cat Training set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# cat Training set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"cat Training set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"cat Training set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "# cat_Testing_AUC\n",
    "cat_Testing_AUC = roc_auc_score(\n",
    "    y_test, cat.predict_proba(x_selected_test)[:, 1])\n",
    "print(\"cat_Testing_AUC:\", cat_Testing_AUC)\n",
    "cat_Testing_AUC_CI = auc_ci(y_true=y_test, y_score=cat.predict_proba(\n",
    "    x_selected_test)[:, 1], positive=1)\n",
    "print(\"cat_Testing_AUC_95%CI:\", cat_Testing_AUC_CI)\n",
    "\n",
    "# cat Testing set Confusion matrix\n",
    "print(\"cat Testing set Confusion matrix：\")\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    y_true=y_test, y_pred=cat.predict(x_selected_test)).ravel()\n",
    "print(\"cat Testing set Confusion matrix:\")\n",
    "print(\"TN={}, FP={}\".format(tn, fp))\n",
    "print(\"FN={}, TP={}\".format(fn, tp))\n",
    "\n",
    "# cat Testing set Sensitivity\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"cat Testing set Sensitivity:\", sensitivity)\n",
    "n_samples = fn + tp\n",
    "ci_sensitivity = sm.proportion_confint(tp, n_samples, alpha=0.05)\n",
    "print(\"cat Testing set 95% CI (sensitivity):\", ci_sensitivity)\n",
    "\n",
    "# cat Testing set Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"cat Testing set Specificity:\", specificity)\n",
    "n_samples = tn + fp\n",
    "ci_specificity = sm.proportion_confint(tn, n_samples, alpha=0.05)\n",
    "print(\"cat Testing set 95% CI (specificity):\", ci_specificity)\n",
    "\n",
    "# cat Testing set accuracy\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "print(\"cat Testing set accuracy:\", accuracy)\n",
    "n_correct = tn + tp\n",
    "n_samples = tn + fp + fn + tp\n",
    "ci_accuracy = sm.proportion_confint(n_correct, n_samples, alpha=0.05)\n",
    "print(\"cat Testing set 95% CI (accuracy):\", ci_accuracy)\n",
    "\n",
    "# CatBoost test ROC curve\n",
    "\n",
    "# auc_cat 95% CI_lower：\n",
    "def auc_cat_ci_lower(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower)\n",
    "\n",
    "\n",
    "AUC_cat_CI_L = auc_cat_ci_lower(y_true=y_test,\n",
    "                                y_score=cat.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"cat_AUC_95%CI_lower:\", AUC_cat_CI_L)\n",
    "\n",
    "# auc_cat 95% CI_upper：\n",
    "def auc_cat_ci_upper(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2 * AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC * (1 - AUC) + (N1 - 1) * (Q1 - AUC**2) + (N2 - 1) *\n",
    "                   (Q2 - AUC**2)) / (N1 * N2))\n",
    "    lower = AUC - 1.96 * SE_AUC\n",
    "    upper = AUC + 1.96 * SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (upper)\n",
    "\n",
    "\n",
    "AUC_cat_CI_U = auc_cat_ci_upper(y_true=y_test,\n",
    "                                y_score=cat.predict_proba(x_selected_test)[:,\n",
    "                                                                           1],\n",
    "                                positive=1)\n",
    "# print(\"cat_AUC_95%CI_upper:\", AUC_cat_CI_U)\n",
    "\n",
    "# ROC\n",
    "y_probs = cat.predict_proba(x_selected_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs[:, 1], pos_label=1)\n",
    "plt.figure(dpi=300)\n",
    "plt.plot(fpr,\n",
    "         tpr,\n",
    "         color='darkorange',\n",
    "         lw=1,\n",
    "         label='AUC = %0.2f' % cat_Testing_AUC + '(%0.2f' % AUC_cat_CI_L + '-' +\n",
    "         '%0.2f)' % AUC_cat_CI_U)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.axis('auto')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('CatBoost ROC Curve', fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\CatBoost_ROC.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost SHAP\n",
    "explainer = shap.TreeExplainer(cat)\n",
    "shap_values = explainer.shap_values(x_selected_test)\n",
    "\n",
    "save_path = 'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\SHAP\\\\'\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "shap.summary_plot(shap_values, x_selected_test, feature_names=x_selected_test.columns, plot_type=\"dot\",\n",
    "                  show=False, plot_size=None, color_bar=True, axis_color=\"#333333\", title=\"cat SHAP Value\")\n",
    "\n",
    "plt.yticks(fontsize=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(save_path + 'cat_shap.png')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost calibration_curve\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = \\\n",
    "    calibration_curve(y_test, cat.predict_proba(x_selected_test)[:, 1], n_bins=4\n",
    "                      #                   , strategy = 'quantile'\n",
    "                      )\n",
    "print(fraction_of_positives)\n",
    "print(mean_predicted_value)\n",
    "\n",
    "plt.plot(mean_predicted_value, fraction_of_positives,\n",
    "         \"rs-\", label='CatBoost Classifier')\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Value\", fontsize=12)\n",
    "plt.ylabel(\"Fraction of Positives\", fontsize=12)\n",
    "plt.title(\"CatBoost Calibration Curve\", fontsize=15)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\CatBoost_cal.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost_DCA\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "\n",
    "y_pred_score = cat.predict_proba(x_selected_test)[:, 1]\n",
    "y_label = y_test\n",
    "thresh_group = np.arange(0, 1, 0.01)\n",
    "net_benefit_model = net_benefit_model(thresh_group, y_pred_score, y_label)\n",
    "net_benefit_all = net_benefit_all(thresh_group, y_label)\n",
    "fig, ax = plt.subplots()\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, net_benefit_all)\n",
    "plt.title('CatBoost Decision Curve Analysis', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\CatBoost_DCA.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise comprision in ensemble models with FDR\n",
    "def ROC_DelongTest(clf_names, clf_methods, x_test, y_label):\n",
    "    len_names = len(clf_names)\n",
    "    len_clfs = len(clf_methods)\n",
    "    results = []\n",
    "    for n, p in zip(range(0, len_names), range(0, len_clfs)):\n",
    "        for i, j in zip(range(n + 1, len_names), range(p + 1, len_clfs)):\n",
    "            y_predict_1 = clf_methods[p].predict_proba(x_test)[:, 1]\n",
    "            y_predict_2 = clf_methods[j].predict_proba(x_test)[:, 1]\n",
    "            print('ROC comparison:', clf_names[n] + ' vs. ' + clf_names[i])\n",
    "            dt = DelongTest(y_predict_1, y_predict_2, y_label)\n",
    "            auc1 = roc_auc_score(y_label, y_predict_1)\n",
    "            auc2 = roc_auc_score(y_label, y_predict_2)\n",
    "            results.append(\n",
    "                [clf_names[n], clf_names[i], auc1, auc2, dt.p_value])\n",
    "            print('\\n')\n",
    "\n",
    "    p_values = [result[-1] for result in results]\n",
    "    _, corrected_p_values, _, _ = multipletests(\n",
    "        p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        results[i][-1] = corrected_p_values[i]\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\n",
    "                      'Model 1', 'Model 2', 'AUC 1', 'AUC 2', 'Adjusted p-value'])\n",
    "    df.to_excel(\n",
    "        r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\\\All_em_ROC.xlsx', index=False)\n",
    "\n",
    "\n",
    "ROCs = ROC_DelongTest(clf_names=[\n",
    "    'RF', 'Bagging', 'AdaBoost', 'GBDT', 'XGBoost', 'LightGBM', 'CatBoost'\n",
    "],\n",
    "    clf_methods=[clf_RF, bag, ada, clf_GBDT, XGB, lgb, cat],\n",
    "    x_test=x_selected_test,\n",
    "    y_label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of pairwise comprision in ensemble models with FDR\n",
    "\n",
    "df_pvalues = pd.read_excel(\n",
    "    r'E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\\\All_em_ROC.xlsx')\n",
    "\n",
    "clf_names = ['RF', 'Bagging', 'AdaBoost',\n",
    "             'GBDT', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "num_models = len(clf_names)\n",
    "p_value_matrix = np.zeros((num_models, num_models))\n",
    "\n",
    "for index, row in df_pvalues.iterrows():\n",
    "    i = clf_names.index(row['Model 1'])\n",
    "    j = clf_names.index(row['Model 2'])\n",
    "    p_value_matrix[i, j] = row['Adjusted p-value']\n",
    "    p_value_matrix[j, i] = row['Adjusted p-value']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.cm.YlGnBu\n",
    "heatmap = ax.imshow(p_value_matrix, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "for i in range(len(clf_names)):\n",
    "    for j in range(len(clf_names)):\n",
    "        if i != j:\n",
    "            p_val = p_value_matrix[i, j]\n",
    "            if p_val < 0.001:\n",
    "                display_text = \"<0.001\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            elif p_val < 0.05:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            else:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'black'\n",
    "                weight = 'normal'\n",
    "            text = ax.text(j, i, display_text,\n",
    "                           ha=\"center\", va=\"center\", color=color, fontweight=weight)\n",
    "\n",
    "ax.set_xticks(np.arange(num_models))\n",
    "ax.set_yticks(np.arange(num_models))\n",
    "ax.set_xticklabels(clf_names, fontsize=12)\n",
    "ax.set_yticklabels(clf_names, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "ax.set_title(\n",
    "    'Adjusted p-values from Delong Test with FDR Correction', fontsize=15)\n",
    "\n",
    "cbar = fig.colorbar(heatmap, ax=ax, shrink=0.56, aspect=30)\n",
    "cbar.ax.set_ylabel('Adjusted p-value', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\all_ensemble_heatmap_FDR_corrected.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise comprision in ensemble and MRI models with FDR\n",
    "\n",
    "models = {'Logit': clf_MRI, 'RF': clf_RF, 'Bagging': bag, 'AdaBoost': ada,\n",
    "          'GBDT': clf_GBDT, 'XGBoost': XGB, 'LightGBM': lgb, 'CatBoost': cat}\n",
    "\n",
    "aucs = {}\n",
    "for name, model in models.items():\n",
    "    if name == 'Logit':\n",
    "        predict = model.predict(x_test_step1[independent_vars])\n",
    "    else:\n",
    "        predict = model.predict_proba(x_selected_test)[:, 1]\n",
    "    aucs[name] = roc_auc_score(y_test, predict)\n",
    "\n",
    "p_values = []\n",
    "comparisons = []\n",
    "for i, (name_1, auc_1) in enumerate(aucs.items()):\n",
    "    for j in range(i+1, len(aucs)):\n",
    "        name_2, auc_2 = list(aucs.items())[j]\n",
    "        if name_1 == 'Logit':\n",
    "            predict_1 = models[name_1].predict(x_test_step1[independent_vars])\n",
    "        else:\n",
    "            predict_1 = models[name_1].predict_proba(x_selected_test)[:, 1]\n",
    "        if name_2 == 'Logit':\n",
    "            predict_2 = models[name_2].predict(x_test_step1[independent_vars])\n",
    "        else:\n",
    "            predict_2 = models[name_2].predict_proba(x_selected_test)[:, 1]\n",
    "        pvalue = DelongTest(predict_1, predict_2, y_test).p_value\n",
    "        p_values.append(pvalue)\n",
    "        comparisons.append((name_1, name_2, auc_1, auc_2, pvalue))\n",
    "\n",
    "rejected, p_values_corrected, _, _ = multipletests(\n",
    "    p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "for i in range(len(comparisons)):\n",
    "    comparisons[i] = comparisons[i][:4] + (p_values_corrected[i],)\n",
    "\n",
    "df = pd.DataFrame(comparisons, columns=[\n",
    "                  'Model 1', 'Model 2', 'AUC 1', 'AUC 2', 'Adjusted p-value'])\n",
    "df.to_excel('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\em_MRI_ROC.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of pairwise comprision in ensemble and MRI models with FDR\n",
    "\n",
    "df_pvalues = pd.read_excel(\n",
    "    r'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\em_MRI_ROC.xlsx')\n",
    "\n",
    "clf_names = ['Logit', 'RF', 'Bagging', 'AdaBoost',\n",
    "             'GBDT', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "num_models = len(clf_names)\n",
    "p_value_matrix = np.zeros((num_models, num_models))\n",
    "\n",
    "for index, row in df_pvalues.iterrows():\n",
    "    i = clf_names.index(row['Model 1'])\n",
    "    j = clf_names.index(row['Model 2'])\n",
    "    p_value_matrix[i, j] = row['Adjusted p-value']\n",
    "    p_value_matrix[j, i] = row['Adjusted p-value']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.cm.YlGnBu\n",
    "heatmap = ax.imshow(p_value_matrix, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "for i in range(len(clf_names)):\n",
    "    for j in range(len(clf_names)):\n",
    "        if i != j:\n",
    "            p_val = p_value_matrix[i, j]\n",
    "            if p_val < 0.001:\n",
    "                display_text = \"<0.001\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            elif p_val < 0.05:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            else:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'black'\n",
    "                weight = 'normal'\n",
    "            text = ax.text(j, i, display_text,\n",
    "                           ha=\"center\", va=\"center\", color=color, fontweight=weight)\n",
    "\n",
    "ax.set_xticks(np.arange(num_models))\n",
    "ax.set_yticks(np.arange(num_models))\n",
    "ax.set_xticklabels(clf_names, fontsize=12)\n",
    "ax.set_yticklabels(clf_names, fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "ax.set_title(\n",
    "    'Adjusted p-values from Delong Test with FDR Correction', fontsize=15)\n",
    "\n",
    "cbar = fig.colorbar(heatmap, ax=ax, shrink=0.56, aspect=30)\n",
    "cbar.ax.set_ylabel('Adjusted p-value', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\em_MRI_heatmap_FDR_corrected.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of ensemble models and MRI model\n",
    "\n",
    "aucs = {'RF':RF_Testing_AUC, 'Bagging':bag_Testing_AUC, 'AdaBoost':AdaBoost_Testing_AUC, 'GBDT':GBDT_Testing_AUC, 'XGBoost':XGB_Testing_AUC, 'LightGBM':lgb_Testing_AUC, 'CatBoost':cat_Testing_AUC, 'Logit': clf_MRI_test_auc}\n",
    "conf_intervals = {'RF':RF_Testing_AUC_CI, 'Bagging':bag_Testing_AUC_CI, 'AdaBoost':AdaBoost_Testing_AUC_CI, 'GBDT':GBDT_Testing_AUC_CI, 'XGBoost':XGB_Testing_AUC_CI, 'LightGBM':lgb_Testing_AUC_CI, 'CatBoost':cat_Testing_AUC_CI, 'Logit': clf_MRI_test_auc_CI}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(\n",
    "    aucs.keys(),\n",
    "    aucs.values(),\n",
    "    color=['#9FE080', 'red', 'green', 'blue', 'orange', 'purple', 'gray', 'pink'],\n",
    "    yerr=[(conf_intervals[m][1] - conf_intervals[m][0]) / 2 for m in aucs.keys()],\n",
    "    capsize=5,\n",
    ")\n",
    "\n",
    "ax.set_title('MRI em AUC Comparisons', fontsize=16)\n",
    "ax.set_xticklabels(aucs.keys(), rotation=45, ha='right', fontsize=12)\n",
    "ax.set_ylabel('AUC', fontsize=12)\n",
    "\n",
    "plt.title('Logit and Ensemble Models', fontsize=15)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Logit_em_C.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NRI and IDI ensemble models VS MRI model\n",
    "\n",
    "models = {'RF': clf_RF, 'Bagging': bag, 'AdaBoost': ada, 'GBDT': clf_GBDT,\n",
    "          'XGBoost': XGB, 'LightGBM': lgb, 'CatBoost': cat, 'clf_MRI': clf_MRI}\n",
    "\n",
    "y_pred_MRI = clf_MRI.predict(x_test_step1[independent_vars])\n",
    "df_MRI = pd.DataFrame(y_pred_MRI, columns=['MRI_pred']).reset_index(drop=True)\n",
    "y_true = y_test\n",
    "y_true = y_test.reset_index(drop=True)\n",
    "\n",
    "# RF\n",
    "y_pred_RF = clf_RF.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_RF = np.where(y_pred_RF == 0, 0.000001, y_pred_RF)\n",
    "y_pred_RF = np.where(y_pred_RF == 1, 0.999999, y_pred_RF)\n",
    "df_RF = pd.DataFrame(y_pred_RF, columns=['RF_pred']).reset_index(drop=True)\n",
    "df_MRI_RF = pd.concat([df_MRI, df_RF, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and RF:', '\\n')\n",
    "RF_nri = nri(df_MRI_RF, 'MRI_pred', 'RF_pred', 'label')\n",
    "print('\\n')\n",
    "RF_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_RF.xlsx\")\n",
    "\n",
    "# bagging\n",
    "y_pred_bagging = bag.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_bagging = np.where(y_pred_bagging == 0, 0.000001, y_pred_bagging)\n",
    "y_pred_bagging = np.where(y_pred_bagging == 1, 0.999999, y_pred_bagging)\n",
    "df_bagging = pd.DataFrame(y_pred_bagging, columns=[\n",
    "                          'bagging_pred']).reset_index(drop=True)\n",
    "df_MRI_bagging = pd.concat([df_MRI, df_bagging, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and bagging:', '\\n')\n",
    "Bagging_nri = nri(df_MRI_bagging, 'MRI_pred', 'bagging_pred', 'label')\n",
    "print('\\n')\n",
    "Bagging_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_bagging.xlsx\")\n",
    "\n",
    "# AdaBoost\n",
    "y_pred_AdaBoost = ada.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_AdaBoost = np.where(y_pred_AdaBoost == 0, 0.000001, y_pred_AdaBoost)\n",
    "y_pred_AdaBoost = np.where(y_pred_AdaBoost == 1, 0.999999, y_pred_AdaBoost)\n",
    "df_AdaBoost = pd.DataFrame(y_pred_AdaBoost, columns=[\n",
    "                           'AdaBoost_pred']).reset_index(drop=True)\n",
    "df_MRI_AdaBoost = pd.concat([df_MRI, df_AdaBoost, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and AdaBoost:', '\\n')\n",
    "AdaBoost_nri = nri(df_MRI_AdaBoost, 'MRI_pred', 'AdaBoost_pred', 'label')\n",
    "print('\\n')\n",
    "AdaBoost_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_AdaBoost.xlsx\")\n",
    "\n",
    "# GBDT\n",
    "y_pred_GBDT = clf_GBDT.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_GBDT = np.where(y_pred_GBDT == 0, 0.000001, y_pred_GBDT)\n",
    "y_pred_GBDT = np.where(y_pred_GBDT == 1, 0.999999, y_pred_GBDT)\n",
    "df_GBDT = pd.DataFrame(y_pred_GBDT, columns=[\n",
    "                       'GBDT_pred']).reset_index(drop=True)\n",
    "df_MRI_GBDT = pd.concat([df_MRI, df_GBDT, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and GBDT:', '\\n')\n",
    "GBDT_nri = nri(df_MRI_GBDT, 'MRI_pred', 'GBDT_pred', 'label')\n",
    "print('\\n')\n",
    "GBDT_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_GBDT.xlsx\")\n",
    "\n",
    "# XGBoost\n",
    "y_pred_XGBoost = XGB.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_XGBoost = np.where(y_pred_XGBoost == 0, 0.000001, y_pred_XGBoost)\n",
    "y_pred_XGBoost = np.where(y_pred_XGBoost == 1, 0.999999, y_pred_XGBoost)\n",
    "df_XGBoost = pd.DataFrame(y_pred_XGBoost, columns=[\n",
    "                          'XGBoost_pred']).reset_index(drop=True)\n",
    "df_MRI_XGBoost = pd.concat([df_MRI, df_XGBoost, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and XGBoost:', '\\n')\n",
    "XGBoost_nri = nri(df_MRI_XGBoost, 'MRI_pred', 'XGBoost_pred', 'label')\n",
    "print('\\n')\n",
    "XGBoost_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_XGBoost.xlsx\")\n",
    "\n",
    "# LightGBM\n",
    "y_pred_LightGBM = lgb.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_LightGBM = np.where(y_pred_LightGBM == 0, 0.000001, y_pred_LightGBM)\n",
    "y_pred_LightGBM = np.where(y_pred_LightGBM == 1, 0.999999, y_pred_LightGBM)\n",
    "df_LightGBM = pd.DataFrame(y_pred_LightGBM, columns=[\n",
    "                           'LightGBM_pred']).reset_index(drop=True)\n",
    "df_MRI_LightGBM = pd.concat([df_MRI, df_LightGBM, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and LightGBM:', '\\n')\n",
    "LightGBM_nri = nri(df_MRI_LightGBM, 'MRI_pred', 'LightGBM_pred', 'label')\n",
    "print('\\n')\n",
    "LightGBM_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_LightGBM.xlsx\")\n",
    "\n",
    "# CatBoost\n",
    "y_pred_CatBoost = cat.predict_proba(x_selected_test)[:, 1]\n",
    "y_pred_CatBoost = np.where(y_pred_CatBoost == 0, 0.000001, y_pred_CatBoost)\n",
    "y_pred_CatBoost = np.where(y_pred_CatBoost == 1, 0.999999, y_pred_CatBoost)\n",
    "df_CatBoost = pd.DataFrame(y_pred_CatBoost, columns=[\n",
    "                           'CatBoost_pred']).reset_index(drop=True)\n",
    "df_MRI_CatBoost = pd.concat([df_MRI, df_CatBoost, y_true], axis=1)\n",
    "print('NRI and IDI of MRI and CatBoost:', '\\n')\n",
    "CatBoost_nri = nri(df_MRI_CatBoost, 'MRI_pred', 'CatBoost_pred', 'label')\n",
    "print('\\n')\n",
    "CatBoost_nri.to_excel(\n",
    "    r\"E:\\Pystudy\\Thyroid_P_N\\data\\ML\\NPM\\emseble\\em_code\\results\\Data_set\\10.39\\NRI_IDI\\NRI_MRI_CatBoost.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise comprision in single, ensemble and MRI models with FDR\n",
    "\n",
    "models = {'Logit': clf_MRI, 'LR': lr, 'KNN': clf_KNN, 'SVM': clf_SVM, 'LDA': clf_LDA, 'GNB': clf_GNB, 'CART': clf_CART, 'RF': clf_RF, 'Bagging': bag, 'AdaBoost': ada, 'GBDT': clf_GBDT, 'XGBoost': XGB, 'LightGBM': lgb, 'CatBoost': cat}\n",
    "\n",
    "aucs = {}\n",
    "for name, model in models.items():\n",
    "    if name == 'Logit':\n",
    "        predict = model.predict(x_test_step1[independent_vars])\n",
    "    else:\n",
    "        predict = model.predict_proba(x_selected_test)[:, 1]\n",
    "    aucs[name] = roc_auc_score(y_test, predict)\n",
    "\n",
    "p_values = []\n",
    "comparisons = []\n",
    "for i, (name_1, auc_1) in enumerate(aucs.items()):\n",
    "    for j in range(i+1, len(aucs)):\n",
    "        name_2, auc_2 = list(aucs.items())[j]\n",
    "        if name_1 == 'Logit':\n",
    "            predict_1 = models[name_1].predict(x_test_step1[independent_vars])\n",
    "        else:\n",
    "            predict_1 = models[name_1].predict_proba(x_selected_test)[:, 1]\n",
    "        if name_2 == 'Logit':\n",
    "            predict_2 = models[name_2].predict(x_test_step1[independent_vars])\n",
    "        else:\n",
    "            predict_2 = models[name_2].predict_proba(x_selected_test)[:, 1]\n",
    "        pvalue = DelongTest(predict_1, predict_2, y_test).p_value\n",
    "        p_values.append(pvalue)\n",
    "        comparisons.append((name_1, name_2, auc_1, auc_2, pvalue))\n",
    "\n",
    "rejected, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "for i in range(len(comparisons)):\n",
    "    comparisons[i] = comparisons[i][:4] + (p_values_corrected[i],)\n",
    "\n",
    "df = pd.DataFrame(comparisons, columns=['Model 1', 'Model 2', 'AUC 1', 'AUC 2', 'Adjusted p-value'])\n",
    "df.to_excel('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\MRI_single_em_roc.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of pairwise comprision in single, ensemble and MRI models with FDR\n",
    "\n",
    "df_pvalues = pd.read_excel(\n",
    "    r'E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\MRI_single_em_roc.xlsx')\n",
    "\n",
    "clf_names = ['Logit', 'LR', 'KNN', 'SVM', 'LDA', 'GNB', 'CART', 'RF',\n",
    "             'Bagging', 'AdaBoost', 'GBDT', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "num_models = len(clf_names)\n",
    "p_value_matrix = np.zeros((num_models, num_models))\n",
    "\n",
    "for index, row in df_pvalues.iterrows():\n",
    "    i = clf_names.index(row['Model 1'])\n",
    "    j = clf_names.index(row['Model 2'])\n",
    "    p_value_matrix[i, j] = row['Adjusted p-value']\n",
    "    p_value_matrix[j, i] = row['Adjusted p-value']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.cm.YlGnBu\n",
    "heatmap = ax.imshow(p_value_matrix, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "for i in range(len(clf_names)):\n",
    "    for j in range(len(clf_names)):\n",
    "        if i != j:\n",
    "            p_val = p_value_matrix[i, j]\n",
    "            if p_val < 0.001:\n",
    "                display_text = \"<0.001\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            elif p_val < 0.05:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'red'\n",
    "                weight = 'bold'\n",
    "            else:\n",
    "                display_text = f\"{p_val:.3f}\"\n",
    "                color = 'black'\n",
    "                weight = 'normal'\n",
    "            text = ax.text(j, i, display_text,\n",
    "                           ha=\"center\", va=\"center\", color=color, fontweight=weight)\n",
    "\n",
    "ax.set_xticks(np.arange(num_models))\n",
    "ax.set_yticks(np.arange(num_models))\n",
    "ax.set_xticklabels(clf_names, fontsize=10)\n",
    "ax.set_yticklabels(clf_names, fontsize=10)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "ax.set_title(\n",
    "    'Adjusted p-values from Delong Test with FDR Correction', fontsize=12)\n",
    "\n",
    "cbar = fig.colorbar(heatmap, ax=ax, shrink=0.56, aspect=30)\n",
    "cbar.ax.set_ylabel('Adjusted p-value', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\single_em_MRI_heatmap_FDR_corrected.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of single, ensemble and MRI models\n",
    "\n",
    "aucs = {'Logit': clf_MRI_test_auc, 'LR': LR_Testing_AUC, 'KNN': KNN_Testing_AUC, 'SVM': SVM_Testing_AUC, 'LDA': LDA_Testing_AUC, 'GNB': GNB_Testing_AUC, 'CART': CART_Testing_AUC, 'RF':RF_Testing_AUC, 'Bagging':bag_Testing_AUC, 'AdaBoost':AdaBoost_Testing_AUC, 'GBDT':GBDT_Testing_AUC, 'XGBoost':XGB_Testing_AUC, 'LightGBM':lgb_Testing_AUC, 'CatBoost':cat_Testing_AUC, }\n",
    "conf_intervals = {'Logit': clf_MRI_test_auc_CI, 'LR': LR_Testing_AUC_CI, 'KNN': KNN_Testing_AUC_CI, 'SVM': SVM_Testing_AUC_CI, 'LDA': LDA_Testing_AUC_CI, 'GNB': GNB_Testing_AUC_CI, 'CART': CART_Testing_AUC_CI, 'RF':RF_Testing_AUC_CI, 'Bagging':bag_Testing_AUC_CI, 'AdaBoost':AdaBoost_Testing_AUC_CI, 'GBDT':GBDT_Testing_AUC_CI, 'XGBoost':XGB_Testing_AUC_CI, 'LightGBM':lgb_Testing_AUC_CI, 'CatBoost':cat_Testing_AUC_CI}\n",
    "\n",
    "import random\n",
    "colors = []\n",
    "for i in range(len(aucs)):\n",
    "    r = lambda: random.randint(0, 255)\n",
    "    colors.append('#%02X%02X%02X' % (r(), r(), r()))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(\n",
    "    aucs.keys(),\n",
    "    aucs.values(),\n",
    "    color=colors,\n",
    "    yerr=[(conf_intervals[m][1] - conf_intervals[m][0]) / 2 for m in aucs.keys()],\n",
    "    capsize=5,\n",
    ")\n",
    "\n",
    "ax.set_title('Logit, Single and Ensemble AUC Comparison', fontsize=12)\n",
    "ax.set_xticklabels(aucs.keys(), rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('AUC', fontsize=12)\n",
    "\n",
    "plt.title('Logit, Single and Ensemble Models', fontsize=12)\n",
    "plt.savefig('E:\\\\Pystudy\\\\Thyroid_P_N\\\\data\\\\ML\\\\NPM\\\\emseble\\\\em_code\\\\results\\\\Data_set\\\\10.39\\\\figures\\\\Logit_Single_Ensemble_C.png', dpi = 300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thyroid_jicheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "781a38685d80fb017f5ea225940c95e671775aff13a458cd171a1bfaa8e26c91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
