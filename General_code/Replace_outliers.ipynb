{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import winsound \n",
    "duration = 500\n",
    "frequency = 440\n",
    "from itertools import combinations\n",
    "import math\n",
    "from math import sqrt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import kstest,shapiro, ttest_ind, mannwhitneyu, chi2_contingency, fisher_exact\n",
    "from statsmodels.formula.api import logit\n",
    "import statsmodels.stats.proportion as sm\n",
    "from statsmodels.tools import add_constant\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import feature_selector as selector\n",
    "from feature_selector import FeatureSelector\n",
    "# from matplotlib import pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font_set = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)\n",
    "import waterfall\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# from ruamel.yaml import timestamp\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFECV, SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, precision_score, make_scorer, log_loss, roc_auc_score, mean_squared_error\n",
    "from pandas import read_csv\n",
    "from pandas import set_option\n",
    "from sklearn import metrics, datasets, clone\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, validation_curve, learning_curve\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "from Delong_test import DelongTest\n",
    "# from pyDeLong import DelongTest\n",
    "from copy import copy\n",
    "from shaphypetune import BoostSearch, BoostRFE, BoostRFA, BoostBoruta\n",
    "from hyperopt import hp\n",
    "from hyperopt import Trials\n",
    "import joblib\n",
    "from keras.models import load_model\n",
    "from DCA import net_benefit_model, net_benefit_all, plot_DCA\n",
    "from AUC_CI import auc_ci\n",
    "from nri_f import nri\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import original data\n",
    "xlsx1_filepath = r'E:\\Pystudy\\Thyroid_P_N\\data\\results\\data_use\\mix\\NHT_mix.xlsx'\n",
    "xlsx2_filepath = r'E:\\Pystudy\\Thyroid_P_N\\data\\results\\data_use\\mix\\PTMC_mix.xlsx'\n",
    "df_1 = pd.read_excel(xlsx1_filepath, sheet_name=1)\n",
    "df_2 = pd.read_excel(xlsx2_filepath, sheet_name=1)\n",
    "\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat data\n",
    "\n",
    "rows_1, _ = df_1.shape\n",
    "rows_2, _ = df_2.shape\n",
    "df_1.insert(0, 'label', [0] * rows_1)\n",
    "df_2.insert(0, 'label', [1] * rows_2)\n",
    "df = pd.concat([df_1, df_2])\n",
    "df = shuffle(df)\n",
    "print(\"\\n\",'original concat：', df.shape)\n",
    "\n",
    "tradition_labes = ['MAD_mm', 'Nodules',\t'Shape', 'Margin',\t'Cystic',\t'ETE',\t'Enhancement',\t'T1_rCNR',\t'T1C_rCNR', 'T1C_increase',\t'T2_rCNR',\t'B800_rCNR',\t'B2000_rCNR',\t'ADC_lesion',\t'ADC_normal',\t'ADC_rCNR',\t'TI_RADS_score']\n",
    "non_radiomics_labels = ['label', 'Name', 'MAD_mm', 'Nodules',\t'Shape', 'Margin',\t'Cystic',\t'ETE',\t'Enhancement',\t'T1_rCNR',\t'T1C_rCNR', 'T1C_increase',\t'T2_rCNR',\t'B800_rCNR',\t'B2000_rCNR',\t'ADC_lesion',\t'ADC_normal',\t'ADC_rCNR',\t'TI_RADS_score']\n",
    "\n",
    "y = df['label']\n",
    "x = df.drop(non_radiomics_labels, axis = 1)\n",
    "colNames = x.columns\n",
    "x = x.astype(np.float64)\n",
    "x = pd.DataFrame(x)\n",
    "x.columns = colNames\n",
    "print('original radiomics：', x.shape, \"\\n\")\n",
    "\n",
    "x_tradition = df[tradition_labes]\n",
    "colName_tradition = x_tradition.columns\n",
    "x_tradition = x_tradition.astype(np.float64)\n",
    "x_tradition = pd.DataFrame(x_tradition)\n",
    "x_tradition.columns = colName_tradition\n",
    "print('original traditional data：', x_tradition.shape, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original mixed data：', df.columns)\n",
    "all_columns = df.columns\n",
    "\n",
    "non_clean_names = ['label', 'Name', 'Nodules',\t'Shape', 'Margin',\t'Cystic',\t'ETE',\t'Enhancement', 'TI_RADS_score']\n",
    "clean_names = all_columns.drop(non_clean_names)\n",
    "print('needed clean columns：', clean_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_outliers_with_median\n",
    "def replace_outliers_with_median(df, column_name, class_column_name):\n",
    "    outlier_counts = {label: 0 for label in df[class_column_name].unique()}\n",
    "    for class_value in df[class_column_name].unique():\n",
    "        class_df = df[df[class_column_name] == class_value]\n",
    "        Q1 = class_df[column_name].quantile(0.25)\n",
    "        Q3 = class_df[column_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        outlier_indices = class_df[(class_df[column_name] < lower_bound) | (class_df[column_name] > upper_bound)].index\n",
    "        outlier_counts[class_value] += len(outlier_indices)\n",
    "        df.loc[outlier_indices, column_name] = class_df[column_name].median()\n",
    "    return df, outlier_counts\n",
    "\n",
    "\n",
    "# replace_outliers_with_median according to label\n",
    "outlier_counts_dict = {label: 0 for label in df['label'].unique()}\n",
    "for col in clean_names:\n",
    "    df, temp_outlier_counts = replace_outliers_with_median(df, col, 'label')    \n",
    "    for label, count in temp_outlier_counts.items():\n",
    "        outlier_counts_dict[label] += count\n",
    "\n",
    "# diaplsy replace_outliers information\n",
    "for label, total_outliers in outlier_counts_dict.items():\n",
    "    total_data_for_label = len(df[df['label'] == label]) * len(clean_names)  \n",
    "    percentage = (total_outliers / total_data_for_label) * 100\n",
    "    print(f'label is{label} in data:')\n",
    "    print(f'Total number of detected outliers: {total_outliers}')\n",
    "    print(f'proportion of processed outliers in the labeled data: {percentage:.2f}%')\n",
    "    print('----------------------------------')\n",
    "        \n",
    "# save\n",
    "df.to_excel(r'E:\\Pystudy\\Thyroid_P_N\\data\\results\\data_use\\mix\\outliers\\non_split_chuyichangzhihou_all1.xlsx', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thyroid_jicheng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "781a38685d80fb017f5ea225940c95e671775aff13a458cd171a1bfaa8e26c91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
